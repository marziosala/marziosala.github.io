---
layout: splash
permalink: /quadric/
title: "Autoencoders"
header:
  overlay_image: /assets/images/quadric/quadric-splash.png
excerpt: "Autoencoders applied to mathematical functions."
---

An *autoencoder* is a type of neural network that can learn efficient representations of data. The learning is unsupervised -- they try to reconstruct the inputs from the inputs themselves.

In its simplest version, it is composed by two neural networks stacked on the top of each other: an *encoder* which compresses the data to a smaller dimensional encoding, and a *decoder*, which tries to reconstruct the original data from this encoding.

Most of the tutorials on autoencoders take images. Indeed, images are a very important application of generative machine learning models; however, here we will work with the reconstruction of a simple mathematical function: a basic quadric, that is parabola of the form
$$
y(x) = \alpha x^2 + \beta.
$$

Our procedure is the following: 
- the parameters $\alpha$ and $\beta$ are sampled in $\mathcal{U}(-1, 1)$, so uniformly between -1 and 1;
- for given values of $\alpha$ and $\beta$, we compute the values $y(x)$ on a (fixed) grid of points.


```python
import seaborn as sns
import torch; torch.manual_seed(0)
import torch.nn as nn
import torch.nn.functional as F
import torch.utils
import torch.distributions
import torchvision
import numpy as np
import matplotlib.pyplot as plt
```


```python
device = 'cpu' #cuda' if torch.cuda.is_available() else 'cpu'
```


```python
uniform_dist = torch.distributions.uniform.Uniform(-2, 2)
```


```python
num_inputs = 10_000
num_points = 100
```


```python
grid = torch.linspace(-1.0, 1.0, num_points)
```


```python
def target_func(x, α, β):
    return α * x**2 + β
```


```python
X, Y = [], []
for _ in range(num_inputs):
    α = uniform_dist.sample()
    β = uniform_dist.sample()
    X.append(target_func(grid, α, β))
    Y.append([α, β])
X = torch.vstack(X)

assert X.shape[0] == num_inputs
assert X.shape[1] == num_points
assert X.isnan().sum() == 0.0
```


```python
for x, (α, β) in zip(X[:5], Y[:5]):
    plt.plot(grid, x.exp(), label=f'α={α.item():.4f}, β={β.item():.4f}')
plt.legend()
```




    <matplotlib.legend.Legend at 0x23844672948>




    
![png](/assets/images/quadric/quadric-1.png)
    



```python
num_hidden = 128
```


```python
class Encoder(nn.Module):
    def __init__(self, latent_dims):
        super(Encoder, self).__init__()
        self.linear1 = nn.Linear(num_points, num_hidden)
        self.linear2 = nn.Linear(num_hidden, latent_dims)

    def forward(self, x):
        x = F.relu(self.linear1(x))
        return self.linear2(x)
```


```python
class Decoder(nn.Module):
    def __init__(self, latent_dims):
        super(Decoder, self).__init__()
        self.linear1 = nn.Linear(latent_dims, num_hidden)
        self.linear2 = nn.Linear(num_hidden, num_points)

    def forward(self, z):
        z = F.relu(self.linear1(z))
        return self.linear2(z)  # can use F.relu() as well as this is a CDF
```


```python
class Autoencoder(nn.Module):
    def __init__(self, latent_dims):
        super(Autoencoder, self).__init__()
        self.encoder = Encoder(latent_dims)
        self.decoder = Decoder(latent_dims)

    def forward(self, x):
        z = self.encoder(x)
        return self.decoder(z)
```


```python
def train(autoencoder, data, epochs=20, lr=1e-3, gamma=0.95):
    optimizer = torch.optim.Adam(autoencoder.parameters(), lr=lr)
    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)
    history = []
    for epoch in range(epochs):
        last_lr = scheduler.get_last_lr()
        total_loss = 0.0
        for x in data:
            x = x.to(device) # GPU
            optimizer.zero_grad()
            x_hat = autoencoder(x)
            loss = ((x - x_hat)**2).sum()
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        scheduler.step()
        print(f"Epoch: {epoch + 1:3d}, lr=: {last_lr[0]:.4e}, total loss: {total_loss:.4f}")
        history.append(total_loss)
    return history
```


```python
%%time
latent_dims = 2
autoencoder = Autoencoder(latent_dims).to(device)
history = train(autoencoder, X, epochs=20, lr=1e-4, gamma=0.99)
```

    Epoch:   0, lr=: 1.0000e-04, total loss: 31960.4691
    Epoch:   1, lr=: 9.9000e-05, total loss: 567.8805
    Epoch:   2, lr=: 9.8010e-05, total loss: 426.9082
    Epoch:   3, lr=: 9.7030e-05, total loss: 405.2072
    Epoch:   4, lr=: 9.6060e-05, total loss: 333.2790
    Epoch:   5, lr=: 9.5099e-05, total loss: 300.5713
    Epoch:   6, lr=: 9.4148e-05, total loss: 279.3894
    Epoch:   7, lr=: 9.3207e-05, total loss: 268.5522
    Epoch:   8, lr=: 9.2274e-05, total loss: 268.2488
    Epoch:   9, lr=: 9.1352e-05, total loss: 232.8506
    Epoch:  10, lr=: 9.0438e-05, total loss: 224.8868
    Epoch:  11, lr=: 8.9534e-05, total loss: 208.0642
    Epoch:  12, lr=: 8.8638e-05, total loss: 185.4336
    Epoch:  13, lr=: 8.7752e-05, total loss: 193.4855
    Epoch:  14, lr=: 8.6875e-05, total loss: 188.1322
    Epoch:  15, lr=: 8.6006e-05, total loss: 167.4567
    Epoch:  16, lr=: 8.5146e-05, total loss: 152.3543
    Epoch:  17, lr=: 8.4294e-05, total loss: 146.6686
    Epoch:  18, lr=: 8.3451e-05, total loss: 155.6928
    Epoch:  19, lr=: 8.2617e-05, total loss: 136.8518
    Wall time: 3min 15s
    


```python
plt.semilogy(history)
```




    [<matplotlib.lines.Line2D at 0x2384339b088>]




    
![png](/assets/images/quadric/quadric-2.png)
    



```python
C = []
for entry in X:
    C.append(autoencoder.encoder(entry.to(device)).cpu().detach().numpy())
C = np.array(C)
```


```python
fig, (ax0, ax1) = plt.subplots(figsize=(10, 4), ncols=2)
sns.histplot(C[:, 0], ax=ax0)
sns.histplot(C[:, 1], ax=ax1)
ax0.set_xlabel('c_1')
ax1.set_xlabel('c_2')
fig.tight_layout()
```


    
![png](/assets/images/quadric/quadric-3.png)
    



```python
from scipy.optimize import minimize
```


```python
α = torch.tensor(2.0) # uniform_dist.sample()
β = torch.tensor(4.0) # uniform_dist.sample()

print(f'Params for the test: α={α.item():.4f}, β={β.item():.4f}')

X_target = target_func(grid, α, β).numpy()

def func(x):
    X_pred = autoencoder.decoder(torch.FloatTensor(x).to(device))
    X_pred = X_pred.cpu().detach().numpy()
    diff = np.linalg.norm(X_target - X_pred).item()
    return diff
```

    Params for the test: α=2.0000, β=4.0000
    


```python
res = minimize(func, [0.0, 0.0], method='Nelder-Mead')
res
```




     final_simplex: (array([[11.75991797, -6.50117183],
           [11.75998447, -6.50119717],
           [11.75996677, -6.50124916]]), array([0.04468434, 0.04468439, 0.04468479]))
               fun: 0.044684335589408875
           message: 'Optimization terminated successfully.'
              nfev: 161
               nit: 85
            status: 0
           success: True
                 x: array([11.75991797, -6.50117183])




```python
X_pred = autoencoder.decoder(torch.FloatTensor(res.x).to(device)).cpu().detach().numpy()
```


```python
fig, (ax0, ax1) = plt.subplots(figsize=(10, 4), ncols=2)
ax0.plot(grid, np.exp(X_target), label=f'α={α.item():.4f}, β={β.item():.4f}')
ax0.plot(grid, np.exp(X_pred), label=f'c1={res.x[0].item():.4f}, c_2={res.x[1].item():.4f}')
ax0.legend()
ax1.plot(grid, np.exp(X_target) - np.exp(X_pred))
ax0.set_xlabel('x')
ax0.set_ylabel('β(x; α, β)')
ax0.set_title('PDF')
ax1.set_xlabel('x')
ax1.set_ylabel('Error')
ax1.set_title('β(x; α, β) - β_NN(x; c_1, c_2)')
fig.tight_layout()
```


    
![png](/assets/images/quadric/quadric-4.png)
    

