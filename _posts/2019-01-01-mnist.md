---
layout: splash
permalink: /mnist/
title: "MNIST Digits"
header:
  image: /assets/images/mnist/MNIST_5_1.png
---

In this post we tackle the quite classical MNIST digit recognition problem. Let's start with a few imports:


```python
import matplotlib.pyplot as plt
import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
from torchvision import datasets, transforms
```

To run the code on both CPUs and GPUs, we setup a `device` variable, automatically set to `cuda` if our system has been configured as such, or to `cpu` otherwise. Using GPUs isn't a gigantic boost in this case, but it's faster and will be useful for more complicated problems.


```python
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu:0')
print(f"Device: {device}")
```

    Device: cuda:0
    

We transform input values in the $[0, 1]$ range into $[-1, 1]$ by subtracting the mean of $0.5$. Values that are zero become -1, and values that are 1 stay at that level. Normalization helps to reduce the skewness of the data. The resize to 28 pixels isn't required with the training and validation data, but will be handy later on to test the network on custom images of different sizes. The variable `batch_size` is set here to 512.


```python
transform = transforms.Compose([transforms.Resize(28, 1),
                                transforms.ToTensor(),
                                transforms.Normalize((0.5,), (0.5,))])
training_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)
validation_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)

batch_size = 512
training_loader = torch.utils.data.DataLoader(dataset=training_dataset, batch_size=batch_size, shuffle=True)
# no need to shuffle the validation data as we don't train on it
validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=batch_size, shuffle=False)
```


```python
def im_convert(tensor):
    image = tensor.clone().detach().numpy()
    image = image.transpose(1, 2, 0)
    image = image * np.array([0.5, 0.5, 0.5]) + np.array([0.5, 0.5, 0.5])
    image = image.clip(0, 1)
    return image
```

To explore the dataset, we plot a few of the images and the corresponding labels.


```python
dataiter = iter(training_loader)
images, labels = dataiter.next()
print(f"Image shape: {images[0].shape[0]} x {images[0].shape[1]} x {images[0].shape[2]}\n")
fig = plt.figure(figsize=(25, 4))

for idx in range(20):
    ax = fig.add_subplot(2, 10, idx + 1, xticks=[], yticks=[])
    plt.imshow(im_convert(images[idx]))
    ax.set_title(f"Label: {labels[idx].item()}")
plt.tight_layout()
```

    Image shape: 1 x 28 x 28
    
    


    
![png](/assets/mnist/MNIST_8_1.png)
    



```python
class LeNet(nn.Module):
    def __init__(self):
        super().__init__()
        # the data on the edges is just dark pixels so it doesn't matter if we lose it
        # hence, no padding, and after each convolutional layer
        # the image size becomes smaller
        # the input layer has 1 layer, 20 features, kernel size is 5, strike is 1
        self.conv1 = nn.Conv2d(1, 20, 5, 1)
        # the second layer has 20 channels and 50 features
        self.conv2 = nn.Conv2d(20, 50, 5, 1)
        # the 50 is the output size of conv2, 4 * 4 is because
        # we have no padding in conv1 (so size is 28 - 4 = 24),
        # then a pooling layer (size 24 / 2 = 12), then conv2
        # (12 - 4 = 8) and another pooling layer (8 / 2 = 4)
        self.fc1 = nn.Linear(4 * 4 * 50, 500)
        self.fc2 = nn.Linear(500, 10)
        # dropout layer
        self.dropout1 = nn.Dropout(0.5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.max_pool2d(x, 2, 2)
        x = F.relu(self.conv2(x))
        x = F.max_pool2d(x, 2, 2)
        # flatten to pass it to the fully connected layer
        x = x.view(-1, 4 * 4 * 50)
        x = F.relu(self.fc1(x))
        x = self.dropout1(x)
        x = self.fc2(x)
        # no activation function because we use the cross entropy loss
        return x 
```


```python
model = LeNet().to(device)
model
```




    LeNet(
      (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))
      (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))
      (fc1): Linear(in_features=800, out_features=500, bias=True)
      (fc2): Linear(in_features=500, out_features=10, bias=True)
      (dropout1): Dropout(p=0.5, inplace=False)
    )




```python
criterion = nn.CrossEntropyLoss().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
```


```python
%%time
epochs = 12
running_loss_history = []
running_corrects_history = []
val_running_loss_history = []
val_running_corrects_history = []

for epoch in range(epochs):
    running_loss = 0.0
    running_corrects = 0
    val_running_loss = 0.0
    val_running_corrects = 0

    for inputs, labels in training_loader:
        inputs = inputs.to(device)
        outputs = inputs.to(device)
        labels = labels.to(device)
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        _, preds = torch.max(outputs, 1)
        running_loss += loss.item()
        running_corrects += torch.sum(preds == labels.data)
    else:
        # validation dataset. Using no_grad() puts to False all the requires_grad flags
        # within the scope of the with
        with torch.no_grad():
            for val_inputs, val_labels in validation_loader:
                val_inputs = val_inputs.to(device)
                val_labels = val_labels.to(device)
                val_outputs = model(val_inputs)
                val_loss = criterion(val_outputs, val_labels)

                _, val_preds = torch.max(val_outputs, 1)
                val_running_loss += val_loss.item()
                val_running_corrects += torch.sum(val_preds == val_labels.data)

        epoch_loss = running_loss / len(training_loader) / batch_size
        running_loss_history.append(epoch_loss)
        running_acc = running_corrects.float() / len(training_loader) / batch_size
        running_corrects_history.append(running_acc)

        val_epoch_loss = val_running_loss / len(validation_loader) / batch_size
        val_running_loss_history.append(val_epoch_loss)
        val_running_acc = val_running_corrects.float() / len(validation_loader) / batch_size
        val_running_corrects_history.append(val_running_acc)

        print(f'Epoch: {epoch}, training loss: {epoch_loss:.4f}, acc: {running_acc:.4f}, val acc: {val_running_acc:.4f}')
```

    Epoch: 0, training loss: 0.0008, acc: 0.8764, val acc: 0.9467
    Epoch: 1, training loss: 0.0002, acc: 0.9663, val acc: 0.9576
    Epoch: 2, training loss: 0.0001, acc: 0.9752, val acc: 0.9636
    Epoch: 3, training loss: 0.0001, acc: 0.9798, val acc: 0.9620
    Epoch: 4, training loss: 0.0001, acc: 0.9811, val acc: 0.9663
    Epoch: 5, training loss: 0.0001, acc: 0.9837, val acc: 0.9653
    Epoch: 6, training loss: 0.0001, acc: 0.9843, val acc: 0.9667
    Epoch: 7, training loss: 0.0000, acc: 0.9848, val acc: 0.9655
    Epoch: 8, training loss: 0.0000, acc: 0.9858, val acc: 0.9669
    Epoch: 9, training loss: 0.0000, acc: 0.9870, val acc: 0.9688
    Epoch: 10, training loss: 0.0000, acc: 0.9878, val acc: 0.9667
    Epoch: 11, training loss: 0.0000, acc: 0.9881, val acc: 0.9681
    Wall time: 1min 55s
    


```python
plt.plot(running_loss_history, label='training loss')
plt.plot(val_running_loss_history, label='validation loss')
plt.legend()
```




    <matplotlib.legend.Legend at 0x29110170400>




    
![png](/assets/mnist/MNIST_13_1.png)
    



```python
plt.plot(running_corrects_history, label='training accuracy')
plt.plot(val_running_corrects_history, label='validation accuracy')
plt.legend()
```




    <matplotlib.legend.Legend at 0x2912392deb0>




    
![png](/assets/mnist/MNIST_14_1.png)
    


Does it work? Let's try on a new image. Open Paint 3D or another similar program and draw a number, then save it to file and pass it to the network. Note that the background should be black and the image white, with other color combinations creating confusion to the model. Non-white colors will be converted to some shade of gray, possibly causing wrong predictions. Also, the image shape should be square to avoid resizing problems. Here I have used a $56 \times 56$ image, resized to $28\times 28$; larger images may not be resized equally well.


```python
from PIL import Image
import PIL.ImageOps
```

The digit I draw is saved in file `five.png` and is shown on the left. We need to remove the color layers by converting it to black-and-white, as well as resize it. The resulting image is shown on the right -- the edges of the number look less sharp.


```python
fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(8, 4))
img = Image.open('./five.png')
ax0.imshow(img)
ax0.set_xticks([])
ax0.set_yticks([])

img = img.convert('1')
img = transform(img)
ax1.imshow(im_convert(img))
ax1.set_xticks([])
ax1.set_yticks([]);
```


    
![png](/assets/mnist/MNIST_18_0.png)
    


To test the model we only need to add an additional dimension, as the model accepts tensors of dimension four, and look for the position of the maximum. 


```python
output = model(img.unsqueeze(1).to(device))
_, pred = torch.max(output, 1)
print(f"Prediction is: {pred.item()}")
```

    Prediction is: 5
    

So it looks like it works! However, it your image has colors, or a different background, or the original image size is much larger than $28 \times 28$, then the model may not work.


```python

```

