---
layout: splash
permalink: /publications/
title: "Publications"
---

```python
import matplotlib.pyplot as plt
import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
from torchvision import datasets, transforms
```


```python
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu:0')
print(f"Device: {device}")
```

    Device: cuda:0
    

We transform input values in the $[0, 1]$ range into $[-1, 1]$ by subtracting the mean of $0.5$. Values that are zero become -1, and values that are 1 stay at that level. Normalization helps to reduce the skewness of the data.


```python
transform = transforms.Compose([transforms.Resize(28, 1),
                                transforms.ToTensor(),
                                transforms.Normalize((0.5,), (0.5,))])
training_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)
validation_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)

batch_size = 512
training_loader = torch.utils.data.DataLoader(dataset=training_dataset, batch_size=batch_size, shuffle=True)
# no need to shuffle the validation data as we don't train on it
validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=batch_size, shuffle=False)
```


```python
def im_convert(tensor):
    image = tensor.clone().detach().numpy()
    image = image.transpose(1, 2, 0)
    image = image * np.array([0.5, 0.5, 0.5]) + np.array([0.5, 0.5, 0.5])
    image = image.clip(0, 1)
    return image
```


```python
dataiter = iter(training_loader)
images, labels = dataiter.next()
print(f"Image shape: {images[0].shape[0]} x {images[0].shape[1]} x {images[0].shape[2]}\n")
fig = plt.figure(figsize=(25, 4))

for idx in range(20):
    ax = fig.add_subplot(2, 10, idx + 1, xticks=[], yticks=[])
    plt.imshow(im_convert(images[idx]))
    ax.set_title(f"Label: {labels[idx].item()}")
plt.tight_layout()
```

    Image shape: 1 x 28 x 28
    
    


    
![png](/assets/mnist/MNIST_5_1.png)
    



```python
## WHAT IS THIS?

class Classifier(nn.Module):
    def __init__(self, D_in, H1, H2, D_out):
        super().__init__()
        self.linear1 = nn.Linear(D_in, H1)
        self.linear2 = nn.Linear(H1, H2)
        self.linear3 = nn.Linear(H2, D_out)
    
    def forward(self, x):
        x = F.relu(self.linear1(x))
        x = F.relu(self.linear2(x))
        x = self.linear3(x)
        return x
```


```python
class LeNet(nn.Module):
    def __init__(self):
        super().__init__()
        # the data on the edges is just dark pixels so it doesn't matter if we lose it
        # hence, no padding, and after each convolutional layer
        # the image size becomes smaller
        # the input layer has 1 layer, 20 features, kernel size is 5, strike is 1
        self.conv1 = nn.Conv2d(1, 20, 5, 1)
        # the second layer has 20 channels and 50 features
        self.conv2 = nn.Conv2d(20, 50, 5, 1)
        # the 50 is the output size of conv2, 4 * 4 is because
        # we have no padding in conv1 (so size is 28 - 4 = 24),
        # then a pooling layer (size 24 / 2 = 12), then conv2
        # (12 - 4 = 8) and another pooling layer (8 / 2 = 4)
        self.fc1 = nn.Linear(4 * 4 * 50, 500)
        self.fc2 = nn.Linear(500, 10)
        # dropout layer
        self.dropout1 = nn.Dropout(0.5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.max_pool2d(x, 2, 2)
        x = F.relu(self.conv2(x))
        x = F.max_pool2d(x, 2, 2)
        # flatten to pass it to the fully connected layer
        x = x.view(-1, 4 * 4 * 50)
        x = F.relu(self.fc1(x))
        x = self.dropout1(x)
        x = self.fc2(x)
        # no activation function because we use the cross entropy loss
        return x 
```


```python
#model = Classifier(784, 125, 65, 10)
model = LeNet().to(device)
model
```




    LeNet(
      (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))
      (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))
      (fc1): Linear(in_features=800, out_features=500, bias=True)
      (fc2): Linear(in_features=500, out_features=10, bias=True)
      (dropout1): Dropout(p=0.5, inplace=False)
    )




```python
criterion = nn.CrossEntropyLoss().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
```


```python
%%time
epochs = 12
running_loss_history = []
running_corrects_history = []
val_running_loss_history = []
val_running_corrects_history = []

for epoch in range(epochs):
    running_loss = 0.0
    running_corrects = 0
    val_running_loss = 0.0
    val_running_corrects = 0

    for inputs, labels in training_loader:
        inputs = inputs.to(device)
        outputs = inputs.to(device)
        labels = labels.to(device)
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        _, preds = torch.max(outputs, 1)
        running_loss += loss.item()
        running_corrects += torch.sum(preds == labels.data)
    else:
        # validation dataset. Using no_grad() puts to False all the requires_grad flags
        # within the scope of the with
        with torch.no_grad():
            for val_inputs, val_labels in validation_loader:
                val_inputs = val_inputs.to(device)
                val_labels = val_labels.to(device)
                val_outputs = model(val_inputs)
                val_loss = criterion(val_outputs, val_labels)

                _, val_preds = torch.max(val_outputs, 1)
                val_running_loss += val_loss.item()
                val_running_corrects += torch.sum(val_preds == val_labels.data)

        epoch_loss = running_loss / len(training_loader) / batch_size
        running_loss_history.append(epoch_loss)
        running_acc = running_corrects.float() / len(training_loader) / batch_size
        running_corrects_history.append(running_acc)

        val_epoch_loss = val_running_loss / len(validation_loader) / batch_size
        val_running_loss_history.append(val_epoch_loss)
        val_running_acc = val_running_corrects.float() / len(validation_loader) / batch_size
        val_running_corrects_history.append(val_running_acc)

        print(f'Epoch: {epoch}, training loss: {epoch_loss:.4f}, acc: {running_acc:.4f}, val acc: {val_running_acc:.4f}')
```

    Epoch: 0, training loss: 0.0008, acc: 0.8764, val acc: 0.9467
    Epoch: 1, training loss: 0.0002, acc: 0.9663, val acc: 0.9576
    Epoch: 2, training loss: 0.0001, acc: 0.9752, val acc: 0.9636
    Epoch: 3, training loss: 0.0001, acc: 0.9798, val acc: 0.9620
    Epoch: 4, training loss: 0.0001, acc: 0.9811, val acc: 0.9663
    Epoch: 5, training loss: 0.0001, acc: 0.9837, val acc: 0.9653
    Epoch: 6, training loss: 0.0001, acc: 0.9843, val acc: 0.9667
    Epoch: 7, training loss: 0.0000, acc: 0.9848, val acc: 0.9655
    Epoch: 8, training loss: 0.0000, acc: 0.9858, val acc: 0.9669
    Epoch: 9, training loss: 0.0000, acc: 0.9870, val acc: 0.9688
    Epoch: 10, training loss: 0.0000, acc: 0.9878, val acc: 0.9667
    Epoch: 11, training loss: 0.0000, acc: 0.9881, val acc: 0.9681
    Wall time: 1min 55s
    


```python
plt.plot(running_loss_history, label='training loss')
plt.plot(val_running_loss_history, label='validation loss')
plt.legend()
```




    <matplotlib.legend.Legend at 0x29110170400>




    
![png](/assets/mnist/MNIST_11_1.png)
    



```python
plt.plot(running_corrects_history, label='training accuracy')
plt.plot(val_running_corrects_history, label='validation accuracy')
plt.legend()
```




    <matplotlib.legend.Legend at 0x2912392deb0>




    
![png](/assets/mnist/MNIST_12_1.png)
    


Does it work? Let's try on a new image. Open Paint 3D or another similar program and draw a number, then save it to file and pass it to the network. Note that the background should be black and the image white, with other color combinations creating confusion to the model. Non-white colors will be converted to some shade of gray, possibly causing wrong predictions. Also, the image shape should be square to avoid resizing problems. Here I have used a $56 \times 56$ image, resized to $28\times 28$; larger images may not be resized equally well.


```python
from PIL import Image
import PIL.ImageOps
```


```python
fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(8, 4))
img = Image.open('./five.png')
ax0.imshow(img)
ax0.set_xticks([])
ax0.set_yticks([])

img = img.convert('1')
img = transform(img)
ax1.imshow(im_convert(img))
ax1.set_xticks([])
ax1.set_yticks([]);
```


    
![png](/assets/mnist/MNIST_15_0.png)
    



```python
output = model(img.unsqueeze(1).to(device))
_, pred = torch.max(output, 1)
print(f"Prediction is: {pred.item()}")
```

    Prediction is: 5
    


```python

```
