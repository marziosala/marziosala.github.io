---
layout: splash
permalink: /acrobot/
title: "Solving the Acrobot Environment with Fitted Q-Iterations"
header:
  overlay_image: /assets/images/acrobot/acrobot.png
excerpt: "Solving the Acrobot environment using Fitted Q-Iterations."
category: reinforcement-learning
---

In this article we look at another method for solving reinforcement learning, [fitted Q-iterations](https://www.jmlr.org/papers/volume6/ernst05a/ernst05a.pdf). Since the Q function is approximated by a neural network, the method is also called Neural Fitted Iterations, or NFI.

We start with a few imports.


```python
import matplotlib.pylab as plt
import torch
from torch import nn
import torch.nn.functional as F
import random
import time
import copy
from collections import deque, namedtuple
import numpy as np
import gym
from tqdm.notebook import tnrange as trange

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
```

The environment is `Acrobot-v0`. Acrobot is a 2-link pendulum with only the second joint actuated.
Initially, both links point downwards. The goal is to swing the
end-effector at a height at least the length of one link above the base.
Both links can swing freely and can pass by each other, i.e., they don't
collide when they have the same angle. The action is either applying -1, 0 or +1 torque on the joint between
the two pendulum links. This problem is not deterministic, as some noise is added to the effect of the torque.
It is a simple problem that can be solved without GPUs on a normal computer in a few minutes, which is quite good for experimenting.


```python
env = gym.make('Acrobot-v1')
num_actions = env.action_space.n
print(f"Environment has {num_actions} actions")
```

    Environment has 3 actions
    


```python
Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))
```


```python
num_episodes = 5_000

total_steps = []
total_rewards = []
data = []

for _ in trange(num_episodes):
    state, done, rewards = env.reset(), False, []
    tmp = []
    while not done:
        action = random.randint(0, num_actions - 1)
        next_state, reward, done, _ = env.step(action)
        tmp.append(Transition(state, np.array([action], dtype=np.int64), next_state, np.array([reward]), done))
        rewards.append(reward)
        state = next_state
    total_rewards.append(sum(rewards))
    total_steps.append(len(rewards))
    # keep only the episodes that have reached the goal
    if total_rewards[-1] > -475:
        data += tmp

print(f"Total number of collected transitions: {len(data)}")
```




    
    Total number of collected transitions: 23459
    


```python
fig, (ax0, ax1) = plt.subplots(figsize=(12, 4), ncols=2)
ax0.plot(total_rewards)
ax0.set_xlabel('Episode')
ax0.set_ylabel('Total Reward')
ax1.plot(total_steps)
ax1.set_xlabel('Episode')
ax1.set_ylabel('# Steps')
```




    Text(0, 0.5, '# Steps')




    
![png](/assets/images/acrobot/acrobot_6_1.png)
    



```python
class Net(nn.Module):
    def __init__(self, input_shape, num_actions):
        super().__init__()
        self.linear1 = nn.Linear(input_shape, 32)
        self.linear2 = nn.Linear(32, 16)
        self.linear3 = nn.Linear(16, num_actions)
    
    def forward(self, x):
        x = torch.relu(self.linear1(x))
        x = torch.relu(self.linear2(x))
        x = self.linear3(x)
        return x
```


```python
class MyDataSet(torch.utils.data.Dataset):
    
    def __init__(self, transitions):
        super().__init__()
        self.transitions = transitions
        
    def __len__(self):
        return len(self.transitions)

    def __getitem__(self, idx):
        return self.transitions[idx]
```


```python
net = Net(env.observation_space.shape[0], num_actions)
dataset = MyDataSet(data)
loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=128, shuffle=True)
loss = nn.MSELoss()
optimizer = torch.optim.Adam(net.parameters(), lr=1e-2)
gamma = 0.95
```


```python
num_epochs = 10
for epoch in range(num_epochs):
    total_diff = 0.0
    for state, action, next_state, reward, done in loader:
        lhs = net(state.float()).gather(1, action)
        rhs = reward.float()
        mask = [not k for k in done]
        rhs[mask] += gamma * net(next_state[mask].float()).max(1)[0].reshape(-1, 1)
        diff = loss(lhs, rhs)
        optimizer.zero_grad()
        diff.backward()
        #torch.nn.utils.clip_grad_norm_(net.parameters(), 1)
        optimizer.step()
        total_diff += diff.item()
    total_diff /= len(loader)
    print(f"Epoch: {epoch + 1}, average total diff = {total_diff}")
```

    Epoch: 1, average total diff = 0.3413930975872537
    Epoch: 2, average total diff = 0.2448757571046767
    Epoch: 3, average total diff = 0.2200017233338693
    Epoch: 4, average total diff = 0.20723383570008952
    Epoch: 5, average total diff = 0.19149739679920932
    Epoch: 6, average total diff = 0.19122742367503437
    Epoch: 7, average total diff = 0.18708374849318163
    Epoch: 8, average total diff = 0.17800687468084303
    Epoch: 9, average total diff = 0.1676326207006755
    Epoch: 10, average total diff = 0.1559177521697205
    


```python
total_rewards = []
for _ in range(100):
    state, done, total_reward = env.reset(), False, 0.0
    while not done:
        action = net(torch.tensor([state], dtype=torch.float)).max(1)[1].item()
        next_state, reward, done, _ = env.step(action)
        total_reward += reward
        state = next_state
    total_rewards.append(total_reward)
```


```python
plt.plot(total_rewards)
plt.title(f'Average reward: {sum(total_rewards) / len(total_rewards)}')
plt.xlabel('Testing Episode')
plt.ylabel('Total Reward')
```




    Text(0, 0.5, 'Total Reward')




    
![png](/assets/images/acrobot/acrobot_12_1.png)
    



```python
from PIL import Image, ImageDraw, ImageFont

state, done = env.reset(), False
frames = [env.render(mode='rgb_array')]
rewards = []
while not done:
    action = net(torch.tensor([state], dtype=torch.float)).max(1)[1].item()
    next_state, reward, done, _ = env.step(action)
    rewards.append(reward)
    frame = env.render(mode='rgb_array')
    image = Image.fromarray(frame)
    draw = ImageDraw.Draw(image)
    font = ImageFont.truetype("font3270.otf ", 24)
    text = f"Step # {len(frames)}\n"
    if action == 0:
        action_as_str = 'negative torque'
    elif action == 1:
        action_as_str = 'no torque'
    else:
        action_as_str = 'positive torque'
    text += f"action: {action_as_str}\n"
    text += f"total reward: {sum(rewards)}"
    draw.multiline_text((10, 10), text, fill=128, font=font)    
    frames.append(np.array(image))
    state = next_state
# add the last image a few more times for a nicer gif
for _ in range(20):
    frames.append(frames[-1])
print(f"Total reward: {sum(rewards)}, # steps = {len(rewards)}")
```

    Total reward: -84.0, # steps = 85
    


```python
from matplotlib import pyplot as plt
from matplotlib import animation

# np array with shape (frames, height, width, channels)
video = np.array(frames[:]) 

fig = plt.figure(figsize=(6, 4))
im = plt.imshow(video[0,:,:,:])
plt.axis('off')
plt.close() # this is required to not display the generated image

def init():
    im.set_data(video[0,:,:,:])

def animate(i):
    im.set_data(video[i,:,:,:])
    return im

anim = animation.FuncAnimation(fig, animate, init_func=init, frames=video.shape[0],
                               interval=100)
```


```python
anim.save('./acrobot-video.gif')
```

<img src='./acrobot-video.gif'/>
