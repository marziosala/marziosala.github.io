---
layout: splash
permalink: /stacking/
title: "On Stacking Models"
header:
  overlay_image: /assets/images/stacking/stacking.jpeg
excerpt: "A quick overview of stacking ensenmble modeling"
---

In this article we will look at **stacking**, or **stacked generalization**, which is a sophisticated ensemble method designed to leverage on the collective strengths of multiple models.

Different machine learning models have different strenghts and often complementary. In some cases there is a clear winner; often, there isn't. Stacking allows us to use many models; its efficacy lies in its ability to mitigate the weaknesses of any single model while amplifying their collective strengths. By leveraging the complementary nature of different algorithms, stacking can achieve superior generalization and predictive accuracy, particularly in complex or high-dimensional datasets.

Stacking represents a paradigm shift from traditional ensemble techniques, such as bagging or boosting. Rather than aggregating predictions through averaging or weighted voting, stacking introduces a hierarchical approach: a **meta-model** is trained to optimally combine the outputs of diverse base models. This meta-model, often a linear regressor or classifier, learns to correct errors and emphasize the most reliable predictions, resulting in a final model that frequently surpasses the performance of its individual components.

A mathematical description of stacking is as follows.

Let $\mathcal{M} = \{m_1, m_2, \dots, m_k\}$ be a set of $k$ base models, where each model $m_i$  is trained on the $n$ samples each with $p$ input features $\mathbf{X} \in \mathbb{R}^{n \times p}$ and produces predictions $\hat{y}_i$​ for the target variable $y \in \mathbb{R}^n$. For each base model $m_i$,

$$
\hat{y}_i = m_i(\mathbf{X}).
$$

The predictions of the base models $\hat{y}_1, \hat{y}_2, \dots, \hat{y}_k$ are combined into a new feature matrix $\mathbf{Z} \in \mathbb{R}^{n \times k}$, where each column represents the predictions of one base model:

$$
\mathbf{Z} = [\hat{y}_1, \hat{y}_2, \dots, \hat{y}_k].
$$

A meta-model $f$ (typically linear regression, logistic regression, or another machine learning algorithm) is trained on the meta-features $\mathbf{Z}$ to produce the final prediction $y^\hat{y}$y,

$$
\hat{y} = f(\mathbf{Z}).
$$

The two components operate on different levels: the base models provide diversity, the meta model allows generation by learning to weight or combine the base models' predictions optimally, often outperforming any single base model. This procedure often corrects the errors of the base models and emphasizes their strengths.

We will implement stacking in two ways. First we start build each individual component following the above procedure, which is useful to learn and understand all the steps involved. Then we use `scikit-learn`'s `StackingRegressor` class; this class performs all the steps internally and it is very easy to use.


```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, KFold
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
```

We will use a small dataset and start with classical exploratory data analysis. 


```python
from sklearn.datasets import fetch_california_housing
housing = fetch_california_housing(as_frame=True)
print(housing.DESCR)
```

    .. _california_housing_dataset:
    
    California Housing dataset
    --------------------------
    
    **Data Set Characteristics:**
    
    :Number of Instances: 20640
    
    :Number of Attributes: 8 numeric, predictive attributes and the target
    
    :Attribute Information:
        - MedInc        median income in block group
        - HouseAge      median house age in block group
        - AveRooms      average number of rooms per household
        - AveBedrms     average number of bedrooms per household
        - Population    block group population
        - AveOccup      average number of household members
        - Latitude      block group latitude
        - Longitude     block group longitude
    
    :Missing Attribute Values: None
    
    This dataset was obtained from the StatLib repository.
    https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html
    
    The target variable is the median house value for California districts,
    expressed in hundreds of thousands of dollars ($100,000).
    
    This dataset was derived from the 1990 U.S. census, using one row per census
    block group. A block group is the smallest geographical unit for which the U.S.
    Census Bureau publishes sample data (a block group typically has a population
    of 600 to 3,000 people).
    
    A household is a group of people residing within a home. Since the average
    number of rooms and bedrooms in this dataset are provided per household, these
    columns may take surprisingly large values for block groups with few households
    and many empty houses, such as vacation resorts.
    
    It can be downloaded/loaded using the
    :func:`sklearn.datasets.fetch_california_housing` function.
    
    .. rubric:: References
    
    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,
      Statistics and Probability Letters, 33 (1997) 291-297
    



```python
df = pd.concat((housing.data, housing.target), axis='columns')
assert df.isnull().sum().sum() == 0
df.sample(5)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>MedInc</th>
      <th>HouseAge</th>
      <th>AveRooms</th>
      <th>AveBedrms</th>
      <th>Population</th>
      <th>AveOccup</th>
      <th>Latitude</th>
      <th>Longitude</th>
      <th>MedHouseVal</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>18298</th>
      <td>10.3894</td>
      <td>32.0</td>
      <td>7.320833</td>
      <td>0.985417</td>
      <td>1583.0</td>
      <td>3.297917</td>
      <td>37.40</td>
      <td>-122.12</td>
      <td>5.00001</td>
    </tr>
    <tr>
      <th>9338</th>
      <td>4.4375</td>
      <td>40.0</td>
      <td>5.735360</td>
      <td>1.103039</td>
      <td>3108.0</td>
      <td>2.303929</td>
      <td>37.99</td>
      <td>-122.61</td>
      <td>2.89600</td>
    </tr>
    <tr>
      <th>19928</th>
      <td>3.6895</td>
      <td>29.0</td>
      <td>5.475000</td>
      <td>0.990909</td>
      <td>1142.0</td>
      <td>2.595455</td>
      <td>36.32</td>
      <td>-119.32</td>
      <td>0.87700</td>
    </tr>
    <tr>
      <th>8091</th>
      <td>3.5054</td>
      <td>33.0</td>
      <td>3.993750</td>
      <td>0.971875</td>
      <td>1157.0</td>
      <td>3.615625</td>
      <td>33.82</td>
      <td>-118.21</td>
      <td>1.46800</td>
    </tr>
    <tr>
      <th>3730</th>
      <td>3.6625</td>
      <td>45.0</td>
      <td>4.850877</td>
      <td>0.986842</td>
      <td>595.0</td>
      <td>2.609649</td>
      <td>34.19</td>
      <td>-118.41</td>
      <td>1.90700</td>
    </tr>
  </tbody>
</table>
</div>




```python
df.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 20640 entries, 0 to 20639
    Data columns (total 9 columns):
     #   Column       Non-Null Count  Dtype  
    ---  ------       --------------  -----  
     0   MedInc       20640 non-null  float64
     1   HouseAge     20640 non-null  float64
     2   AveRooms     20640 non-null  float64
     3   AveBedrms    20640 non-null  float64
     4   Population   20640 non-null  float64
     5   AveOccup     20640 non-null  float64
     6   Latitude     20640 non-null  float64
     7   Longitude    20640 non-null  float64
     8   MedHouseVal  20640 non-null  float64
    dtypes: float64(9)
    memory usage: 1.4 MB



```python
import matplotlib.pylab as plt
import seaborn as sns
plt.figure(figsize=(8,5))
sns.histplot(df.MedHouseVal,bins=50,kde=True)
plt.title("Distribution of House Price")
plt.xlabel("House of Price($) ");
```


    
![png](/assets/images/stacking/stacking-1.png)
    



```python
plt.figure(figsize=(8,5))
sns.heatmap(df.select_dtypes(include='number').corr(),annot=True,cmap='coolwarm')
plt.title("Correlation Between Features");
```


    
![png](/assets/images/stacking/stacking-2.png)
    



```python
plt.figure(figsize=(8,5))
sns.scatterplot(data=df, x='MedInc', y='MedHouseVal')
plt.title("Median Income Vs House Price");
```


    
![png](/assets/images/stacking/stacking-3.png)
    



```python
plt.figure(figsize=(20, 5))
sns.boxplot(data=df, x='HouseAge', y='MedHouseVal')
plt.title("House Age Vs House Price");
```


    
![png](/assets/images/stacking/stacking-4.png)
    



```python
plt.figure(figsize=(8,5))
sns.scatterplot(data=df, x='Latitude', y = 'Longitude', hue='MedHouseVal')
```




    <Axes: xlabel='Latitude', ylabel='Longitude'>




    
![png](/assets/images/stacking/stacking-5.png)
    



```python
df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>MedInc</th>
      <th>HouseAge</th>
      <th>AveRooms</th>
      <th>AveBedrms</th>
      <th>Population</th>
      <th>AveOccup</th>
      <th>Latitude</th>
      <th>Longitude</th>
      <th>MedHouseVal</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>8.3252</td>
      <td>41.0</td>
      <td>6.984127</td>
      <td>1.023810</td>
      <td>322.0</td>
      <td>2.555556</td>
      <td>37.88</td>
      <td>-122.23</td>
      <td>4.526</td>
    </tr>
    <tr>
      <th>1</th>
      <td>8.3014</td>
      <td>21.0</td>
      <td>6.238137</td>
      <td>0.971880</td>
      <td>2401.0</td>
      <td>2.109842</td>
      <td>37.86</td>
      <td>-122.22</td>
      <td>3.585</td>
    </tr>
    <tr>
      <th>2</th>
      <td>7.2574</td>
      <td>52.0</td>
      <td>8.288136</td>
      <td>1.073446</td>
      <td>496.0</td>
      <td>2.802260</td>
      <td>37.85</td>
      <td>-122.24</td>
      <td>3.521</td>
    </tr>
    <tr>
      <th>3</th>
      <td>5.6431</td>
      <td>52.0</td>
      <td>5.817352</td>
      <td>1.073059</td>
      <td>558.0</td>
      <td>2.547945</td>
      <td>37.85</td>
      <td>-122.25</td>
      <td>3.413</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3.8462</td>
      <td>52.0</td>
      <td>6.281853</td>
      <td>1.081081</td>
      <td>565.0</td>
      <td>2.181467</td>
      <td>37.85</td>
      <td>-122.25</td>
      <td>3.422</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>20635</th>
      <td>1.5603</td>
      <td>25.0</td>
      <td>5.045455</td>
      <td>1.133333</td>
      <td>845.0</td>
      <td>2.560606</td>
      <td>39.48</td>
      <td>-121.09</td>
      <td>0.781</td>
    </tr>
    <tr>
      <th>20636</th>
      <td>2.5568</td>
      <td>18.0</td>
      <td>6.114035</td>
      <td>1.315789</td>
      <td>356.0</td>
      <td>3.122807</td>
      <td>39.49</td>
      <td>-121.21</td>
      <td>0.771</td>
    </tr>
    <tr>
      <th>20637</th>
      <td>1.7000</td>
      <td>17.0</td>
      <td>5.205543</td>
      <td>1.120092</td>
      <td>1007.0</td>
      <td>2.325635</td>
      <td>39.43</td>
      <td>-121.22</td>
      <td>0.923</td>
    </tr>
    <tr>
      <th>20638</th>
      <td>1.8672</td>
      <td>18.0</td>
      <td>5.329513</td>
      <td>1.171920</td>
      <td>741.0</td>
      <td>2.123209</td>
      <td>39.43</td>
      <td>-121.32</td>
      <td>0.847</td>
    </tr>
    <tr>
      <th>20639</th>
      <td>2.3886</td>
      <td>16.0</td>
      <td>5.254717</td>
      <td>1.162264</td>
      <td>1387.0</td>
      <td>2.616981</td>
      <td>39.37</td>
      <td>-121.24</td>
      <td>0.894</td>
    </tr>
  </tbody>
</table>
<p>20640 rows × 9 columns</p>
</div>




```python
X = housing.data
y = housing.target.values

# X = X[y < 5]
# y = y[y < 5]
print(f"The dataset has {X.shape[0]:,} rows and {X.shape[1]} columns")

print(f"Task: Predict median house value (in $100,000s)")
print(f"\nFeatures:")
for col in X.columns:
    print(f"  - {col}")
print(f"\nTarget statistics:")
print(f"  Mean: ${y.mean():.2f} (×100k) = ${y.mean()*100:.0f}k")
print(f"  Median: ${np.median(y):.2f} (×100k) = ${np.median(y) * 100:.0f}k")
print(f"  Std: ${y.std():.2f} (×100k)")
print(f"  Min: ${y.min():.2f} (×100k)")
print(f"  Max: ${y.max():.2f} (×100k)\n")
```

    The dataset has 20,640 rows and 8 columns
    Task: Predict median house value (in $100,000s)
    
    Features:
      - MedInc
      - HouseAge
      - AveRooms
      - AveBedrms
      - Population
      - AveOccup
      - Latitude
      - Longitude
    
    Target statistics:
      Mean: $2.07 (×100k) = $207k
      Median: $1.80 (×100k) = $180k
      Std: $1.15 (×100k)
      Min: $0.15 (×100k)
      Max: $5.00 (×100k)
    



```python
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```


```python
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)
print(f"# train rows: {len(X_train):,}, # test rows: {len(X_test):,}")
```

    # train rows: 16,512, # test rows: 4,128



```python
rf = RandomForestRegressor(n_estimators=100, random_state=42)
gb = GradientBoostingRegressor(n_estimators=100, random_state=42)
```


```python
kf = KFold(n_splits=5, shuffle=True, random_state=42)
rf_oof_pred = np.zeros_like(y_train, dtype=float)
gb_oof_pred = np.zeros_like(y_train, dtype=float)

for train_idx, val_idx in kf.split(X_train):
    X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]
    y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]

    rf.fit(X_train_fold, y_train_fold)
    gb.fit(X_train_fold, y_train_fold)

    rf_oof_pred[val_idx] = rf.predict(X_val_fold)
    gb_oof_pred[val_idx] = gb.predict(X_val_fold)

X_train_meta = np.column_stack((rf_oof_pred, gb_oof_pred))

meta_model = LinearRegression()
_ = meta_model.fit(X_train_meta, y_train)
```


```python
rf.fit(X_train, y_train)
gb.fit(X_train, y_train);
```


```python
rf_pred_test = rf.predict(X_test)
gb_pred_test = gb.predict(X_test)
X_test_meta = np.column_stack((rf_pred_test, gb_pred_test))
meta_pred = meta_model.predict(X_test_meta)
```


```python
rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred_test))
gb_rmse = np.sqrt(mean_squared_error(y_test, gb_pred_test))
meta_rmse = np.sqrt(mean_squared_error(y_test, meta_pred))

print('RMSE:')
print(f"- random forest: {rf_rmse:.3f}")
print(f"- gradient boosting: {gb_rmse:.3f}")
print(f"- stacked model: {meta_rmse:.3f}")
```

    RMSE:
    - random forest: 0.506
    - gradient boosting: 0.542
    - stacked model: 0.504



```python
rf_rmse = r2_score(y_test, rf_pred_test)
gb_rmse = r2_score(y_test, gb_pred_test)
meta_rmse = r2_score(y_test, meta_pred)

print('R2 score:')
print(f"- random forest: {rf_rmse:.3f}")
print(f"- gradient boosting: {gb_rmse:.3f}")
print(f"- stacked model: {meta_rmse:.3f}")
```

    R2 score:
    - random forest: 0.805
    - gradient boosting: 0.776
    - stacked model: 0.806



```python

```


```python
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.svm import SVR
```


```python
base_models = [
    ('ridge', Ridge()),
    ('lasso', Lasso()),
    ('elastic', ElasticNet(alpha=1.0, l1_ratio=0.5)),
    ('rf', RandomForestRegressor(n_estimators=100, random_state=42)),
    ('gb', GradientBoostingRegressor(n_estimators=100, random_state=42)),
    ('svr', SVR(kernel='rbf', C=1.0))
]
meta_model = LinearRegression()
```


```python
for name, model in base_models:
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
        
    print(f"{name} model: RMSE = {rmse:.3f}, MAE = {mae:.3f}, R² = {r2:.3f}")
```

    ridge model: RMSE = 0.746, MAE = 0.533, R² = 0.576
    lasso model: RMSE = 1.145, MAE = 0.906, R² = -0.000
    elastic model: RMSE = 1.021, MAE = 0.805, R² = 0.204
    rf model: RMSE = 0.506, MAE = 0.328, R² = 0.805
    gb model: RMSE = 0.542, MAE = 0.372, R² = 0.776
    svr model: RMSE = 0.596, MAE = 0.398, R² = 0.729



```python
stacking_reg = StackingRegressor(
    estimators=base_models,
    final_estimator=meta_model,
    cv=5,
    n_jobs=-1
)

stacking_reg.fit(X_train, y_train)
y_pred_stack = stacking_reg.predict(X_test)

rmse = np.sqrt(mean_squared_error(y_test, y_pred_stack))
mae = mean_absolute_error(y_test, y_pred_stack)
r2 = r2_score(y_test, y_pred_stack)

print(f"Stacking model: RMSE = {rmse:.3f}, MAE = {mae:.3f}, R² = {r2:.3f}")
```

    Stacking model: RMSE = 0.499, MAE = 0.328, R² = 0.810

