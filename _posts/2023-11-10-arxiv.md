---
layout: splash
permalink: /arxiv/
title: "Embeddings on arXiv Articles"
header:
  overlay_image: /assets/images/arxiv/arxiv-splash.jpeg
excerpt: "Looking for similar articles using OpenAI's text embedding models"
---

In this article we look at [embeddings](https://en.wikipedia.org/wiki/Word_embedding) with the aim of estimating the similarity between different texts. An embedding is a vector representation of a word or a text. The idea is to transform a set of words $T$ into a mathematical object $x \in \mathbb{R}^n$, with $n$ large, say a thousand or more. Crucially, the value $n$ is the same for any $T$, irrespective of the number of words that compose it; this makes it possible to compare different texts using simple vector operations.

As concrete use case, we look at the articles published on [arxiv.org](https://www.arxiv.org); the embeddings are provided by [OpenAI](https://www.openai.com). Our goal is to detect articles that are similar to a given one.

The requires packages are the usual suspects; the only additions are the [openai](https://github.com/openai/openai-python) and the [arxiv](https://github.com/lukasschwab/arxiv.py) python packages, both easily installable with `pip`.


```python
import arxiv
from datetime import datetime
from IPython.display import display, HTML
import matplotlib.colors as mcolors
import matplotlib.pylab as plt
import numpy as np
import pandas as pd
from pathlib import Path
import openai
from openai.embeddings_utils import cosine_similarity, get_embedding
import pickle
import requests
from sklearn.cluster import KMeans

secret_key = Path('./secret-key.txt').open('r').read()
openai.api_key = secret_key
```

Of the many models OpenAI has published, here we are interested in *text* models. There are several of them, some marked as `similarity` and others as `embedding`. The difference is that `similarity` models tend to be used for applications where two pieces of text are compared to get a similarity score as output, while `embedding` models generally take an input query and then return the top $K$ best matches from a collection of texts that was embedded into a database previously. Therefore, for what we are doing here we need `similarity`.


```python
models = openai.Model.list()
models = pd.DataFrame(models['data']).set_index('id')
models.created = pd.to_datetime(models.created, unit='s')
models[models.index.str.contains('text')].sort_values('created', ascending=False)[:10]
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>object</th>
      <th>created</th>
      <th>owned_by</th>
    </tr>
    <tr>
      <th>id</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>text-embedding-ada-002</th>
      <td>model</td>
      <td>2022-12-16 19:01:39</td>
      <td>openai-internal</td>
    </tr>
    <tr>
      <th>text-davinci-003</th>
      <td>model</td>
      <td>2022-11-28 01:40:35</td>
      <td>openai-internal</td>
    </tr>
    <tr>
      <th>ada-code-search-text</th>
      <td>model</td>
      <td>2022-04-28 19:01:50</td>
      <td>openai-dev</td>
    </tr>
    <tr>
      <th>text-search-babbage-doc-001</th>
      <td>model</td>
      <td>2022-04-28 19:01:49</td>
      <td>openai-dev</td>
    </tr>
    <tr>
      <th>text-search-babbage-query-001</th>
      <td>model</td>
      <td>2022-04-28 19:01:49</td>
      <td>openai-dev</td>
    </tr>
    <tr>
      <th>text-search-curie-doc-001</th>
      <td>model</td>
      <td>2022-04-28 19:01:49</td>
      <td>openai-dev</td>
    </tr>
    <tr>
      <th>text-search-curie-query-001</th>
      <td>model</td>
      <td>2022-04-28 19:01:49</td>
      <td>openai-dev</td>
    </tr>
    <tr>
      <th>babbage-code-search-text</th>
      <td>model</td>
      <td>2022-04-28 19:01:49</td>
      <td>openai-dev</td>
    </tr>
    <tr>
      <th>code-search-babbage-text-001</th>
      <td>model</td>
      <td>2022-04-28 19:01:47</td>
      <td>openai-dev</td>
    </tr>
    <tr>
      <th>code-search-ada-text-001</th>
      <td>model</td>
      <td>2022-04-28 19:01:47</td>
      <td>openai-dev</td>
    </tr>
  </tbody>
</table>
</div>



OpenAI [suggests](https://openai.com/blog/new-and-improved-embedding-model) to use `text-embedding-ada-002`, so this is what we will use. 
According to OpenAI, this new model unifies the specific models for text similarity, text search and code search into a single new model, and performs better than the previous embedding models across a diverse set of text search, sentence similarity, and code search benchmarks. The price at the time of writing is about 3,000 pages per dollar, which is quite cheap. An alternative would be `text-similarity-ada-001`, not displayed in the list above because released in April 2022.

Since our scope is to search for similar articles published on arxiv.org, we first need to download the salient features of the articles. We will limit ourselve to title and summary, although a deeper analysis would also focus on the article content (which needs extracing from the PDF or from the source files). The nice [arxiv](https://github.com/lukasschwab/arxiv.py) package provides an easy-to-use API for doing exactly that. The `query()` function below simply queries for the specified number of articles, sortd by update date, and returns a list with the provided metadata.


```python
def query(query, max_results, offset: int = 0):
    search = arxiv.Search(
        query=query,
        max_results=max_results,
        sort_by=arxiv.SortCriterion.LastUpdatedDate
    )
    client = arxiv.Client()
    return list(client.results(search=search, offset=offset))
```

As a first check, we query the last 50 articles in six [categories](https://arxiv.org/category_taxonomy) that are quite different from each other: astrophysics, economics, condensed matter, quantitative biology and quantitative finance. In general an article on astrophysics should have little in common with one on quantitative finance or condensed matter. Overlaps may exists between economics and quantitative finance, and mathematics is used in all other sciences. To make things a bit more difficult, arxiv has a single main category as well as many secondary ones, and the query for a certain category returns articles that have that category as either the main or one of the secondary ones, so the distinction may not always be clear-cut. All considered, though, we should be able to cluster them quite nicely


```python
results = []
for cat in ['astro-ph', 'econ', 'cond-mat', 'math', 'q-bio', 'q-fin']:
    results += query(f'cat:{cat}.*', 50, offset=0)
```

We use title and summary to compute the embedding. It takes about a minute to call `get_embedding()` for the 300 articles of our tests; once computed, embeddings are stored in a dataframe together with the title.


```python
embeddings = []
for result in results:
    embeddings.append(get_embedding(result.summary.replace('\n', ' '), "text-embedding-ada-002"))
```


```python
def assemble_df(results, embeddings):
    assert len(results) == len(embeddings)
    get_main_cat = lambda x: x if '.' not in x else x[:x.find('.')]
    return pd.DataFrame({
        'embedding': embeddings,
        'title': [r.title for r in results],
        'summary': [r.summary for r in results],
        'date': [r.published for r in results],
        'entry_id': [r.entry_id for r in results],
        'primary_category': [get_main_cat(r.primary_category) for r in results],
        'categories': [[get_main_cat(c) for c in r.categories] for r in results]
    })
```


```python
df = assemble_df(results, embeddings)
```

The embeddings have a dimension of 1536.


```python
print(f"# dimensions: {len(df.embedding.values[0])}")
```

    # dimensions: 1536
    

As we expect six clusters we choose `num_clusters=6`. As suggested by [OpenAI](https://platform.openai.com/docs/guides/embeddings/use-cases), we use [K-means clustering](https://en.wikipedia.org/wiki/K-means_clustering) and [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) for reducing the dimensionality to two, making visualizations possible.


```python
matrix = np.vstack(df.embedding.values)
num_clusters = 6
kmeans = KMeans(n_clusters=num_clusters, init="k-means++", random_state=42)
kmeans.fit(matrix)
labels = kmeans.labels_
df["cluster"] = labels
```


```python
from sklearn.manifold import TSNE
import matplotlib
import matplotlib.pyplot as plt

fig = plt.figure(figsize=(8, 8))
tsne = TSNE(n_components=2, perplexity=15, random_state=42, init="random", learning_rate=200)
vis_dims2 = tsne.fit_transform(matrix)

x = [x for x, y in vis_dims2]
y = [y for x, y in vis_dims2]

for category, color in enumerate(['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown']):
    xs = np.array(x)[df.cluster == category]
    ys = np.array(y)[df.cluster == category]
    plt.scatter(xs, ys, color=color, alpha=0.4, label=f'cluster #{category}')
    plt.scatter(xs.mean(), ys.mean(), marker="s", color=color, s=150)
plt.title("2D t-SNE Clusters")
fig.tight_layout()
plt.legend();
```


    
![png](/assets/images/arxiv/arxiv-1.png)
    


Results aren't too bad, with cluster 0 being astrophysics, cluster 4 condensed matter, and cluster 5 mathematics. Clusters 1, 2 and 3, instead are a bit more mixed up; from the picture above, clusters 1 and 3 are quite interconnected. 


```python
for cluster in range(6):
    print(f'Cluster {cluster}:', end='')
    subset = df[df.cluster == cluster]
    freq = pd.Series(subset.primary_category.tolist()).value_counts()
    for cat, count in freq[:4].items():
        print(f' {cat}: {count}', end='')
    print()
```

    Cluster 0: astro-ph: 39 gr-qc: 6 hep-th: 1
    Cluster 1: econ: 30 cs: 6 q-bio: 4 math: 3
    Cluster 2: q-bio: 18 cs: 15 econ: 6 quant-ph: 2
    Cluster 3: econ: 24 q-fin: 19 math: 5 cs: 5
    Cluster 4: cond-mat: 37 physics: 9 q-bio: 3 nucl-th: 1
    Cluster 5: math: 36 cond-mat: 4 econ: 3 q-bio: 3
    

We are now ready to apply the method on articles for which we don't know the answer: here we take the last 2,000 articles in the *quantitative finance* category, *mathematical finance* subcategory. This covers the articles published from 2016 to 2023.


```python
results = query("cat:q-fin.mf", 2_000)
```

The five most recent ones are the following:


```python
def to_html(results):
    text = "<table><tr><th>Publication Date</th><th></th><th style='text-align:left'>Authors and Title</th></tr>"
    for r in results:
        authors = ', '.join(a.name for a in r.authors)
        categories = ', '.join(r.categories)
        text += f"<tr><td width=100>{datetime.strftime(r.published, '%Y-%m-%d')}</td>" \
            + f"<td width=20><a style='text-decoration: none;' href='{r.entry_id}'>ðŸ”—</a></td>" \
            + f"<td style='text-align:left'><tt>{categories}</tt><br>{authors}<br><b>{r.title}</b></td></tr>"
    text += "</table>"
    display(HTML(text))


to_html(results[0:5])
```


<table><tr><th>Publication Date</th><th></th><th style='text-align:left'>Authors and Title</th></tr><tr><td width=100>2023-11-08</td><td width=20><a style='text-decoration: none;' href='http://arxiv.org/abs/2311.04841v1'>ðŸ”—</a></td><td style='text-align:left'><tt>q-fin.MF</tt><br>Gechun Liang, Moris Strub, Yuwei Wang<br><b>Predictable relative forward performance processes: Multi-agent and mean field games for portfolio management</b></td></tr><tr><td width=100>2023-11-08</td><td width=20><a style='text-decoration: none;' href='http://arxiv.org/abs/2311.04727v1'>ðŸ”—</a></td><td style='text-align:left'><tt>q-fin.ST, q-fin.MF, q-fin.TR</tt><br>Siu Hin Tang, Mathieu Rosenbaum, Chao Zhou<br><b>Forecasting Volatility with Machine Learning and Rough Volatility: Example from the Crypto-Winter</b></td></tr><tr><td width=100>2023-08-15</td><td width=20><a style='text-decoration: none;' href='http://arxiv.org/abs/2308.07763v2'>ðŸ”—</a></td><td style='text-align:left'><tt>q-fin.PM, cs.CE, q-fin.MF</tt><br>Purushottam Parthasarathy, Avinash Bhardwaj, Manjesh K. Hanawal<br><b>Online Universal Dirichlet Factor Portfolios</b></td></tr><tr><td width=100>2023-11-06</td><td width=20><a style='text-decoration: none;' href='http://arxiv.org/abs/2311.03538v1'>ðŸ”—</a></td><td style='text-align:left'><tt>q-fin.MF, q-fin.CP, q-fin.PR, 91G80, 62P05, 60G40, 35R35</tt><br>Anne Mackay, Marie-Claude Vachon<br><b>On an Optimal Stopping Problem with a Discontinuous Reward</b></td></tr><tr><td width=100>2023-01-04</td><td width=20><a style='text-decoration: none;' href='http://arxiv.org/abs/2301.01555v2'>ðŸ”—</a></td><td style='text-align:left'><tt>q-fin.MF, 91B16, 91G10, 60H30</tt><br>Leonid Dolinskyi, Yan Dolinsky<br><b>Optimal Liquidation with High Risk Aversion and Small Linear Price Impact</b></td></tr></table>


We will repeat the procedure done above: we compute the embedding for each result and store it into a dataframe, then use K-means to cluster them.


```python
embeddings = []
for result in results:
    embeddings.append(get_embedding(result.summary, "text-similarity-ada-001"))
```


```python
df = assemble_df(results, embeddings)
matrix = np.vstack(df.embedding.values)
```


```python
num_clusters = 9
kmeans = KMeans(n_clusters=num_clusters, init="k-means++", random_state=42)
kmeans.fit(matrix)
labels = kmeans.labels_
df["cluster"] = labels
```

We don't visualize but rather print a few articles for each cluster.


```python
for cluster in range(num_clusters):
    print(f'â‡’ cluster #{cluster}')
    for i, row in df[df.cluster == cluster][:5].iterrows():
        print(row.title)
    print()
```

    â‡’ cluster #0
    On intermediate Marginals in Martingale Optimal Transportation
    Superhedging duality for multi-action options under model uncertainty with information delay
    Some asymptotics for short maturity Asian options
    Multi-period static hedging of European options
    Is (independent) subordination relevant in option pricing?
    
    â‡’ cluster #1
    Maximal Martingale Wasserstein Inequality
    Transportation-cost inequalities for non-linear Gaussian functionals
    Symmetric Bernoulli distributions and minimal dependence copulas
    Global universal approximation of functional input maps on weighted spaces
    Kelvin Waves, Klein-Kramers and Kolmogorov Equations, Path-Dependent Financial Instruments: Survey and New Results
    
    â‡’ cluster #2
    Forecasting Volatility with Machine Learning and Rough Volatility: Example from the Crypto-Winter
    Black-Litterman Asset Allocation under Hidden Truncation Distribution
    Discrete $q$-exponential limit order cancellation time distribution
    Neural Network for valuing Bitcoin options under jump-diffusion and market sentiment model
    Uncovering Market Disorder and Liquidity Trends Detection
    
    â‡’ cluster #3
    Risk of Transfer Learning and its Applications in Finance
    Simulation schemes for the Heston model with Poisson conditioning
    Fitted Value Iteration Methods for Bicausal Optimal Transport
    Calibration of Derivative Pricing Models: a Multi-Agent Reinforcement Learning Perspective
    Signature Methods in Stochastic Portfolio Theory
    
    â‡’ cluster #4
    On an Optimal Stopping Problem with a Discontinuous Reward
    Relative Arbitrage Opportunities in an Extended Mean Field System
    Valuation of a Financial Claim Contingent on the Outcome of a Quantum Measurement
    Mean Field Games for Optimal Investment Under Relative Performance Criteria
    Unwinding Stochastic Order Flow: When to Warehouse Trades
    
    â‡’ cluster #5
    Predictable relative forward performance processes: Multi-agent and mean field games for portfolio management
    Optimal Liquidation with High Risk Aversion and Small Linear Price Impact
    Benchmark Beating with the Increasing Convex Order
    Optimal exercise decision of American options under model uncertainty
    Bounding the approach to oligarchy in a variant of the yard-sale model
    
    â‡’ cluster #6
    Bank Deposits as {\em Money Quanta}
    Endogenous Distress Contagion in a Dynamic Interbank Model
    Systemic robustness: a mean-field particle system approach
    Propagation of carbon tax in credit portfolio through macroeconomic factors
    Leverage, Endogenous Unbalanced Growth, and Asset Price Bubbles
    
    â‡’ cluster #7
    A Heath-Jarrow-Morton framework for energy markets: a pragmatic approach
    Power law in Sandwiched Volterra Volatility model
    The birth of (a robust) Arbitrage Theory in de Finetti's early contributions
    From constant to rough: A survey of continuous volatility modeling
    Aggregation of financial markets
    
    â‡’ cluster #8
    Online Universal Dirichlet Factor Portfolios
    Estimation of VaR with jump process: application in corn and soybean markets
    Optimal fees in hedge funds with first-loss compensation
    A Unified Formula of the Optimal Portfolio for Piecewise Hyperbolic Absolute Risk Aversion Utilities
    A Bonus-Malus Framework for Cyber Risk Insurance and Optimal Cybersecurity Provisioning
    
    

A better and more useful way of testing the similarity is to look for similar articles. We select an article that sits in between deep hedging and synthetic market data generation and search for other similar articles.


```python
selected_title = 'A Data-driven Market Simulator for Small Data Environments'
selected_id = df.index[df.title == selected_title].values[0]
print(f'Cluster: {df.loc[selected_id].cluster}\nTitle: {df.loc[selected_id].title}\nSummary: {df.loc[selected_id].summary}')
```

    Cluster: 3
    Title: A Data-driven Market Simulator for Small Data Environments
    Summary: Neural network based data-driven market simulation unveils a new and flexible
    way of modelling financial time series without imposing assumptions on the
    underlying stochastic dynamics. Though in this sense generative market
    simulation is model-free, the concrete modelling choices are nevertheless
    decisive for the features of the simulated paths. We give a brief overview of
    currently used generative modelling approaches and performance evaluation
    metrics for financial time series, and address some of the challenges to
    achieve good results in the latter. We also contrast some classical approaches
    of market simulation with simulation based on generative modelling and
    highlight some advantages and pitfalls of the new approach. While most
    generative models tend to rely on large amounts of training data, we present
    here a generative model that works reliably in environments where the amount of
    available training data is notoriously small. Furthermore, we show how a rough
    paths perspective combined with a parsimonious Variational Autoencoder
    framework provides a powerful way for encoding and evaluating financial time
    series in such environments where available training data is scarce. Finally,
    we also propose a suitable performance evaluation metric for financial time
    series and discuss some connections of our Market Generator to deep hedging.
    


```python
selected = df.loc[selected_id].embedding
df['similarity'] = df.embedding.apply(lambda x: cosine_similarity(x, selected))
```


```python
for i, row in df.sort_values('similarity', ascending=False)[:30].iterrows():
    if i == selected_id:
        continue
    print(f"{str(row.date)[:10]} {row.entry_id} (#{row.cluster}@{row.similarity:.3f}) {row.title}")
```

    2020-07-08 http://arxiv.org/abs/2007.04154v1 (#3@0.933) Robust pricing and hedging via neural SDEs
    2019-11-05 http://arxiv.org/abs/1911.01700v1 (#3@0.926) Deep Hedging: Learning to Simulate Equity Option Markets
    2021-03-21 http://arxiv.org/abs/2103.11435v3 (#3@0.920) A deep learning approach to data-driven model-free pricing and to martingale optimal transport
    2022-03-07 http://arxiv.org/abs/2203.03179v3 (#2@0.917) Detecting data-driven robust statistical arbitrage strategies with deep neural networks
    2020-05-30 http://arxiv.org/abs/2006.00218v2 (#3@0.916) Sig-SDEs model for quantitative finance
    2023-06-19 http://arxiv.org/abs/2306.16422v1 (#3@0.915) Neural networks can detect model-free static arbitrage strategies
    2019-07-15 http://arxiv.org/abs/1907.06673v2 (#3@0.913) Quant GANs: Deep Generation of Financial Time Series
    2023-08-29 http://arxiv.org/abs/2308.15135v2 (#2@0.908) Signature Trading: A Path-Dependent Extension of the Mean-Variance Framework with Exogenous Signals
    2022-06-13 http://arxiv.org/abs/2206.06109v2 (#4@0.908) Markov Decision Processes under Model Uncertainty
    2023-03-14 http://arxiv.org/abs/2303.07925v10 (#3@0.908) Deep incremental learning models for financial temporal tabular datasets with distribution shifts
    2020-11-05 http://arxiv.org/abs/2011.02870v2 (#2@0.908) Model-free Analysis of Dynamic Trading Strategies
    2021-04-19 http://arxiv.org/abs/2104.10483v2 (#2@0.907) Adaptive learning for financial markets mixing model-based and model-free RL for volatility targeting
    2020-11-09 http://arxiv.org/abs/2011.04804v2 (#3@0.907) Nonparametric Adaptive Bayesian Stochastic Control Under Model Uncertainty
    2019-04-23 http://arxiv.org/abs/1904.10523v1 (#3@0.907) A neural network-based framework for financial model calibration
    2019-01-28 http://arxiv.org/abs/1901.09647v2 (#3@0.907) Deep Learning Volatility
    2021-10-22 http://arxiv.org/abs/2110.11848v1 (#3@0.906) Clustering Market Regimes using the Wasserstein Distance
    2021-11-25 http://arxiv.org/abs/2111.13164v6 (#3@0.905) Neural network stochastic differential equation models with applications to financial data forecasting
    2023-06-27 http://arxiv.org/abs/2306.15835v1 (#3@0.905) Non-parametric online market regime detection and regime clustering for multidimensional and path-dependent data structures
    2019-01-27 http://arxiv.org/abs/1901.09309v4 (#4@0.905) High-dimensional statistical arbitrage with factor models and stochastic control
    2021-12-14 http://arxiv.org/abs/2112.07335v1 (#3@0.902) Deep Partial Hedging
    2023-10-02 http://arxiv.org/abs/2310.01285v1 (#3@0.902) Automated regime detection in multidimensional time series data using sliced Wasserstein k-means clustering
    2022-02-02 http://arxiv.org/abs/2202.00941v2 (#2@0.902) CTMSTOU driven markets: simulated environment for regime-awareness in trading policies
    2021-12-13 http://arxiv.org/abs/2112.06823v1 (#0@0.902) Multi-Asset Spot and Option Market Simulation
    2020-02-19 http://arxiv.org/abs/2002.08492v2 (#3@0.902) Equal Risk Pricing of Derivatives with Deep Hedging
    2022-07-02 http://arxiv.org/abs/2207.00739v1 (#3@0.902) Deep Learning for Systemic Risk Measures
    2020-11-09 http://arxiv.org/abs/2011.04256v1 (#3@0.901) A bivariate Normal Inverse Gaussian process with stochastic delay: efficient simulations and applications to energy markets
    2022-12-20 http://arxiv.org/abs/2212.09957v1 (#3@0.901) Beyond Surrogate Modeling: Learning the Local Volatility Via Shape Constraints
    2022-04-06 http://arxiv.org/abs/2204.02891v3 (#2@0.901) Stochastic volatility modeling of high-frequency CSI 300 index and dynamic jump prediction driven by machine learning
    2022-10-17 http://arxiv.org/abs/2210.09331v1 (#0@0.901) Measure-valued processes for energy markets
    

The similarity level is always quite high, above 0.90, suggesting that perhaps the texts above are all quite similar in general terms, so similarity alone may not suffice. Still, we manage to find relevant articles, as one could see by looking at the references in the one we have selected or in the list of articles that cite it, or by the publications of the authors. References, citations and authors are easily available and in a real-life applications we would surely use them to increase the precision of the suggestions, together with the content from the article.
