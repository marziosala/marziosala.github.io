---
layout: splash
permalink: /arxiv/
title: "Embeddings on arXiv Articles"
header:
  overlay_image: /assets/images/arxiv/arxiv-splash.jpeg
excerpt: "Looking for similar articles using OpenAI's text embedding models"
---

In this article we look at [embeddings](https://en.wikipedia.org/wiki/Word_embedding) with the aim of estimating the similarity between different texts. An embedding is a vector representation of a word or a text. The idea is to transform a set of words $T$ into a mathematical object $x \in \mathbb{R}^n$, with $n$ large, say a thousand or more. Crucially, the value $n$ is the same for any $T$, irrespective of the number of words that compose it; this makes it possible to compare different texts using simple vector operations.

As concrete use case, we look at the articles published on [arxiv.org](https://www.arxiv.org); the embeddings are provided by [OpenAI](https://www.openai.com). Our goal is to detect articles that are similar to a given one.

The requires packages are the usual suspects; the only additions are the [openai](https://github.com/openai/openai-python) and the [arxiv](https://github.com/lukasschwab/arxiv.py) python packages, both easily installable with `pip`.


```python
import arxiv
from datetime import datetime
from IPython.display import display, HTML
import json
import matplotlib.colors as mcolors
import matplotlib.pylab as plt
import numpy as np
import pandas as pd
from pathlib import Path
import openai
from openai.embeddings_utils import cosine_similarity, get_embedding
import pickle
import requests
from sklearn.cluster import KMeans, AgglomerativeClustering
```

Of the many models OpenAI has published, here we are interested in *text* models. There are several of them, some marked as `similarity` and others as `embedding`. The difference is that `similarity` models tend to be used for applications where two pieces of text are compared to get a similarity score as output, while `embedding` models generally take an input query and then return the top $K$ best matches from a collection of texts that was embedded into a database previously. Therefore, for what we are doing here we need `similarity`.


```python
models = openai.Model.list()
models = pd.DataFrame(models['data']).set_index('id')
models.created = pd.to_datetime(models.created, unit='s')
models[models.index.str.contains('text')].sort_values('created', ascending=False)[:10]
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>object</th>
      <th>created</th>
      <th>owned_by</th>
    </tr>
    <tr>
      <th>id</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>text-embedding-ada-002</th>
      <td>model</td>
      <td>2022-12-16 19:01:39</td>
      <td>openai-internal</td>
    </tr>
    <tr>
      <th>text-davinci-003</th>
      <td>model</td>
      <td>2022-11-28 01:40:35</td>
      <td>openai-internal</td>
    </tr>
    <tr>
      <th>ada-code-search-text</th>
      <td>model</td>
      <td>2022-04-28 19:01:50</td>
      <td>openai-dev</td>
    </tr>
    <tr>
      <th>text-search-babbage-doc-001</th>
      <td>model</td>
      <td>2022-04-28 19:01:49</td>
      <td>openai-dev</td>
    </tr>
    <tr>
      <th>text-search-babbage-query-001</th>
      <td>model</td>
      <td>2022-04-28 19:01:49</td>
      <td>openai-dev</td>
    </tr>
    <tr>
      <th>text-search-curie-doc-001</th>
      <td>model</td>
      <td>2022-04-28 19:01:49</td>
      <td>openai-dev</td>
    </tr>
    <tr>
      <th>text-search-curie-query-001</th>
      <td>model</td>
      <td>2022-04-28 19:01:49</td>
      <td>openai-dev</td>
    </tr>
    <tr>
      <th>babbage-code-search-text</th>
      <td>model</td>
      <td>2022-04-28 19:01:49</td>
      <td>openai-dev</td>
    </tr>
    <tr>
      <th>code-search-babbage-text-001</th>
      <td>model</td>
      <td>2022-04-28 19:01:47</td>
      <td>openai-dev</td>
    </tr>
    <tr>
      <th>code-search-ada-text-001</th>
      <td>model</td>
      <td>2022-04-28 19:01:47</td>
      <td>openai-dev</td>
    </tr>
  </tbody>
</table>
</div>



OpenAI [suggests](https://openai.com/blog/new-and-improved-embedding-model) to use `text-embedding-ada-002`, so this is what we will use. 
According to OpenAI, this new model unifies the specific models for text similarity, text search and code search into a single new model, and performs better than the previous embedding models across a diverse set of text search, sentence similarity, and code search benchmarks. The price at the time of writing is about 3,000 pages per dollar, which is quite cheap. An alternative would be `text-similarity-ada-001`, not displayed in the list above because released in April 2022.

Since our scope is to search for similar articles published on arxiv.org, we first need to download the salient features of the articles. We will limit ourselve to title and summary, although a deeper analysis would also focus on the article content (which needs extracing from the PDF or from the source files). The nice [arxiv](https://github.com/lukasschwab/arxiv.py) package provides an easy-to-use API for doing exactly that. The `query()` function below simply queries for the specified number of articles, sortd by update date, and returns a list with the provided metadata.


```python
def query(query, max_results, offset: int = 0):
    search = arxiv.Search(
        query=query,
        max_results=max_results,
        sort_by=arxiv.SortCriterion.LastUpdatedDate
    )
    client = arxiv.Client()
    return list(client.results(search=search, offset=offset))
```

As a first check, we query the last 50 articles in six [categories](https://arxiv.org/category_taxonomy) that are quite different from each other: astrophysics, economics, condensed matter, quantitative biology and quantitative finance. In general an article on astrophysics should have little in common with one on quantitative finance or condensed matter. Overlaps may exists between economics and quantitative finance, and mathematics is used in all other sciences. To make things a bit more difficult, arxiv has a single main category as well as many secondary ones, and the query for a certain category returns articles that have that category as either the main or one of the secondary ones, so the distinction may not always be clear-cut. All considered, though, we should be able to cluster them quite nicely


```python
results = []
for cat in ['astro-ph', 'econ', 'cond-mat', 'math', 'q-bio', 'q-fin']:
    results += query(f'cat:{cat}.*', 50, offset=0)
```

We use title and summary to compute the embedding. It takes about a minute to call `get_embedding()` for the 300 articles of our tests; once computed, embeddings are stored in a dataframe together with the title. The embedding is defined as the title plus the summary.


```python
def get_text(result):
    return result.title + ' ' + result.summary.replace('\n', ' ')
```


```python
embeddings = []
for result in results:
    embeddings.append(get_embedding(get_text(result), "text-embedding-ada-002"))
```


```python
def assemble_df(results, embeddings):
    assert len(results) == len(embeddings)
    get_main_cat = lambda x: x if '.' not in x else x[:x.find('.')]
    return pd.DataFrame({
        'embedding': embeddings,
        'title': [r.title for r in results],
        'summary': [r.summary for r in results],
        'date': [r.published for r in results],
        'entry_id': [r.entry_id for r in results],
        'primary_category': [get_main_cat(r.primary_category) for r in results],
        'categories': [[get_main_cat(c) for c in r.categories] for r in results]
    })
```


```python
df = assemble_df(results, embeddings)
```

The embeddings have a dimension of 1536.


```python
print(f"# dimensions: {len(df.embedding.values[0])}")
```

    # dimensions: 1536
    

As we expect six clusters we choose `num_clusters=6`. As suggested by [OpenAI](https://platform.openai.com/docs/guides/embeddings/use-cases), we use [K-means clustering](https://en.wikipedia.org/wiki/K-means_clustering) and [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) for reducing the dimensionality to two, making visualizations possible.


```python
matrix = np.vstack(df.embedding.values)
num_clusters = 6
kmeans = KMeans(n_clusters=num_clusters, init="k-means++", random_state=42)
kmeans.fit(matrix)
labels = kmeans.labels_
df["cluster"] = labels
```


```python
from sklearn.manifold import TSNE
import matplotlib
import matplotlib.pyplot as plt

fig = plt.figure(figsize=(8, 8))
tsne = TSNE(n_components=2, perplexity=15, random_state=42, init="random", learning_rate=200)
vis_dims2 = tsne.fit_transform(matrix)

x = [x for x, y in vis_dims2]
y = [y for x, y in vis_dims2]

labels = {}
for cluster in range(6):
    subset = df[df.cluster == cluster]
    freq = pd.Series(subset.primary_category.tolist()).value_counts()
    labels[cluster] = f'#{cluster}' + ', '.join([f' {cat} ({count})' for cat, count in freq[:4].items()])
labels

for cluster, color in enumerate(['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown']):
    xs = np.array(x)[df.cluster == cluster]
    ys = np.array(y)[df.cluster == cluster]
    plt.scatter(xs, ys, color=color, alpha=0.4, label=labels[cluster])
    plt.scatter(xs.mean(), ys.mean(), marker="s", color=color, s=150)
plt.title("2D t-SNE Clusters")
fig.tight_layout()
plt.legend();
```


    
![png](/assets/images/arxiv/arxiv-1.png)
    


Results aren quite good, with cluster #0 (mathematics), cluster #3 (condensed matter) and cluster #4 (astrophysics) quite distinct. Cluster #1 (quantitative biology) is a bit dispersed over other clusters, while clusters #2 and #5 have some overlap. 

We are now ready to apply the method on articles for which we don't know the answer: here we take all the articles in the *quantitative finance* category with the word *volatility* in the title. This covers about 950 articles published from 1997 to 2023.


```python
results = query("cat:q-fin.* AND ti:volatility", None)
print(f"Found {len(results)} results.")
```

    Found 947 results.
    

The five most recent ones are the following:


```python
def to_html(results):
    text = "<table><tr><th>Publication Date</th><th></th><th style='text-align:left'>Authors and Title</th></tr>"
    for r in results:
        authors = ', '.join(a.name for a in r.authors)
        categories = ', '.join(r.categories)
        text += f"<tr><td width=100>{datetime.strftime(r.published, '%Y-%m-%d')}</td>" \
            + f"<td width=20><a style='text-decoration: none;' href='{r.entry_id}'>ðŸ”—</a></td>" \
            + f"<td style='text-align:left'><tt>{categories}</tt><br>{authors}<br><b>{r.title}</b></td></tr>"
    text += "</table>"
    display(HTML(text))
```


```python
to_html(results[0:5])
```


<table><tr><th>Publication Date</th><th></th><th style='text-align:left'>Authors and Title</th></tr><tr><td width=100>2022-04-21</td><td width=20><a style='text-decoration: none;' href='http://arxiv.org/abs/2204.10103v3'>ðŸ”—</a></td><td style='text-align:left'><tt>q-fin.MF, math.PR, 91G20, 60H30, 60F10, 60G22</tt><br>Giacomo Giorgio, Barbara Pacchiarotti, Paolo Pigato<br><b>Short-time asymptotics for non self-similar stochastic volatility models</b></td></tr><tr><td width=100>2023-11-08</td><td width=20><a style='text-decoration: none;' href='http://arxiv.org/abs/2311.04727v1'>ðŸ”—</a></td><td style='text-align:left'><tt>q-fin.ST, q-fin.MF, q-fin.TR</tt><br>Siu Hin Tang, Mathieu Rosenbaum, Chao Zhou<br><b>Forecasting Volatility with Machine Learning and Rough Volatility: Example from the Crypto-Winter</b></td></tr><tr><td width=100>2021-12-09</td><td width=20><a style='text-decoration: none;' href='http://arxiv.org/abs/2201.07880v2'>ðŸ”—</a></td><td style='text-align:left'><tt>q-fin.CP, econ.EM</tt><br>Zhe Wang, Nicolas Privault, Claude Guet<br><b>Deep self-consistent learning of local volatility</b></td></tr><tr><td width=100>2023-07-27</td><td width=20><a style='text-decoration: none;' href='http://arxiv.org/abs/2307.15718v2'>ðŸ”—</a></td><td style='text-align:left'><tt>q-fin.ST, q-fin.PR, q-fin.RM</tt><br>Darsh Kachhara, John K. E Markin, Astha Singh<br><b>Option Smile Volatility and Implied Probabilities: Implications of Concavity in IV Curves</b></td></tr><tr><td width=100>2023-11-02</td><td width=20><a style='text-decoration: none;' href='http://arxiv.org/abs/2311.01228v1'>ðŸ”—</a></td><td style='text-align:left'><tt>q-fin.MF, math.PR, 91G30, 60H10, 60H35, 60G22</tt><br>Giulia Di Nunno, Anton Yurchenko-Tytarenko<br><b>Power law in Sandwiched Volterra Volatility model</b></td></tr></table>


As done before, we compute the embeddings using title and summary. 


```python
embeddings = []
for result in results:
    embeddings.append(get_embedding(get_text(result), "text-embedding-ada-002"))
```


```python
import pickle
with open('data.pickle', 'wb') as f:
    pickle.dump((results, embeddings), f)
```


```python
import pickle
with open('data.pickle', 'rb') as f:
    results, embeddings = pickle.load(f)
```


```python
df = assemble_df(results, embeddings)
matrix = np.vstack(df.embedding.values)
```

Since we don't know how many clusters we have, we use the [agglomerative clustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html) method.


```python
ac = AgglomerativeClustering(n_clusters=None, distance_threshold=1.5) 
ac.fit(matrix)
labels = ac.labels_
df["cluster"] = labels
num_clusters = ac.n_clusters_
```

Instead of the t-SNE visualization, which will add little to our case, we use `gpt-4` to obtain a list of keywords that describe the clusters. We do that by collecting all the titles, without the summaries to make the request shorter, for each cluster and calling `openai.ChatCompletion.create()`.


```python
def get_completion(prompt, model="gpt-3.5-turbo-16k", temperature=0):
    messages = [{"role": "user", "content": prompt}]
    response = openai.ChatCompletion.create(
        model=model,
        messages=messages,
        temperature=temperature,
    )
    return response.usage.total_tokens, response.choices[0].message["content"]
```


```python
topics = {}
for cluster in range(num_clusters):
    titles = '\n\n'.join([t for t in df[df.cluster == cluster].title])
    prompt = f"""
    You are an expert in quantitative finance and you are helping reviewing the
    titles of scientific articles.
    What are the 5 most important topics of the following titles of scientific articles.
    Return the answers as json that contains all the topics in a list under the tag "topics".

    {titles}"""
    _, response = get_completion(prompt)

    topics[cluster] = json.loads(response)['topics']
```

We report the topics as suggested by the LLM together with a few titles of the articles in the cluster.


```python
html = "<table>"
for cluster, topic in topics.items():
    t = f'#{cluster} ' + ', '.join(topic)
    articles = '<li>'.join([row.title for _, row in df[df.cluster == cluster][:10].iterrows()])
    html += f"<tr><td><b>{t} </b><ul><li>{articles} </ul></td></tr>"
html += "</table>"
display(HTML(html))
```


<table>
<tr><td><b>Cluster 0: Cryptocurrency market dynamics, Stock market volatility, GDP prediction, Monetary policy announcements, European carbon and energy prices </b><ul><li>Bayesian framework for characterizing cryptocurrency market dynamics, structural dependency, and volatility using potential field</li><li>The Effect of COVID-19 on Cryptocurrencies and the Stock Market Volatility -- A Two-Stage DCC-EGARCH Model Analysis</li><li>Harnessing the Potential of Volatility: Advancing GDP Prediction</li><li>Volatility jumps and the classification of monetary policy announcements<li>Modeling Volatility and Dependence of European Carbon and Energy Prices<li>Nowcasting Stock Implied Volatility with Twitter</li><li>Ask "Who", Not "What": Bitcoin Volatility Forecasting with Twitter Data</li><li>Role of Variable Renewable Energy Penetration on Electricity Price and its Volatility Across Independent System Operators in the United States</li><li>Investor base and idiosyncratic volatility of cryptocurrencies</li><li>FX Resilience around the World: Fighting Volatile Cross-Border Capital Flows</li></ul></td></tr><tr><td><b>Cluster 1: Stochastic volatility models, Rough volatility, Option pricing, Volatility estimation, Model calibration </b><ul><li>Short-time asymptotics for non self-similar stochastic volatility models</li><li>Power law in Sandwiched Volterra Volatility model</li><li>From constant to rough: A survey of continuous volatility modeling</li><li>Thiele's PIDE for unit-linked policies in the Heston-Hawkes stochastic volatility model</li><li>Instabilities of Super-Time-Stepping Methods on the Heston Stochastic Volatility Model</li><li>Convergence of the Euler--Maruyama particle scheme for a regularised McKean--Vlasov equation arising from the calibration of local-stochastic volatility models</li><li>A stochastic volatility model for the valuation of temperature derivatives</li><li>Estimating the roughness exponent of stochastic volatility from discrete observations of the realized variance</li><li>A lower bound for the volatility swap in the lognormal SABR model</li><li>Statistical inference for rough volatility: Central limit theorems</li></ul></td></tr>
<tr><td><b>Cluster 2: Return and Volatility Modeling, Algorithmic Trading, News-driven Expectations, Bitcoin Volatility, Volatility Models </b><ul><li>A Modeling Approach of Return and Volatility of Structured Investment Products with Caps and Floors</li><li>Sizing Strategies for Algorithmic Trading in Volatile Markets: A Study of Backtesting and Risk Mitigation Analysis</li><li>News-driven Expectations and Volatility Clustering</li><li>Bitcoin Volatility and Intrinsic Time Using Double Subordinated Levy Processes</li><li>Spatial and Spatiotemporal Volatility Models: A Review<li>VolTS: A Volatility-based Trading System to forecast Stock Markets Trend using Statistics and Machine Learning</li><li>Liquidity Premium, Liquidity-Adjusted Return and Volatility, and a Unified Modern Portfolio Theory: illustrated with Crypto Assets</li><li>Systemic risk indicator based on implied and realized volatility<li>On the Guyon-Lekeufack Volatility Model</li><li>Dynamic Risk Measurement by EVT based on Stochastic Volatility models via MCMC</li></ul></td></tr><tr><td><b>Cluster 3: Asian Options, Implied Volatility, Stochastic Volatility Models, Option Pricing, Local Volatility Models </b><ul><li>Asymptotics for Short Maturity Asian Options in a Jump-Diffusion model with Local Volatility<li>On the implied volatility of European and Asian call options under the stochastic volatility Bachelier model<li>On the implied volatility of Asian options under stochastic volatility models<li>Dark Matter in (Volatility and) Equity Option Risk Premiums<li>Approximate Pricing of Derivatives Under Fractional Stochastic Volatility Model<li>Option Pricing with Time-Varying Volatility Risk Aversion<li>Hull and White and AlÃ²s type formulas for barrier options in stochastic volatility models with nonzero correlation<li>Pricing Path-dependent Options under Stochastic Volatility via Mellin Transform<li>Optimal market completion through financial derivatives with applications to volatility risk<li>Toward an efficient hybrid method for pricing barrier options on assets with stochastic volatility </ul></td></tr><tr><td><b>Cluster 4: Option pricing, Implied volatility, Volatility smile, Stochastic volatility models, Arbitrage-free models </b><ul><li>Option Smile Volatility and Implied Probabilities: Implications of Concavity in IV Curves<li>On the skew and curvature of implied and local volatilities<li>Building arbitrage-free implied volatility: Sinkhorn's algorithm and variants<li>Extreme ATM skew in a local volatility model with discontinuity: joint density approach<li>The quintic Ornstein-Uhlenbeck volatility model that jointly calibrates SPX and VIX smiles<li>Sparse modeling approach to the arbitrage-free interpolation of plain-vanilla option prices and implied volatilities<li>Adaptive Gradient Descent Methods for Computing Implied Volatility<li>Tighter 'Uniform Bounds for Black-Scholes Implied Volatility' and the applications to root-finding<li>Joint SPX-VIX calibration with Gaussian polynomial volatility models: deep pricing with quantization hints<li>Reconstructing Volatility: Pricing of Index Options under Rough Volatility </ul></td></tr><tr><td><b>Cluster 5: Volatility forecasting, Machine learning, Deep learning, Neural networks, Financial time series </b><ul><li>Forecasting Volatility with Machine Learning and Rough Volatility: Example from the Crypto-Winter<li>Co-Training Realized Volatility Prediction Model with Neural Distributional Transformation</li><li>DeepVol: A Pre-Trained Universal Asset Volatility Model</li><li>Combining Deep Learning and GARCH Models for Financial Volatility and Risk Forecasting</li><li>Stock Volatility Prediction Based on Transformer Model Using Mixed-Frequency Data</li><li>Introducing the Ïƒ-Cell: Unifying GARCH, Stochastic Fluctuations and Evolving Mechanisms in RNN-based Volatility Forecasting</li><li>Recurrent Neural Networks with more flexible memory: better predictions than rough volatility</li><li>Graph Neural Networks for Forecasting Multivariate Realized Volatility with Spillover Effects</li><li>Comparing Deep Learning Models for the Task of Volatility Prediction Using Multivariate Data</li><li>Short-Term Volatility Prediction Using Deep CNNs Trained on Order Flow </ul></td>
</tr><tr><td><b>Cluster 6: Deep learning, Calibration, Local volatility models, Stochastic volatility models, Implied volatility surfaces </b><ul></li><li>Deep self-consistent learning of local volatility</li><li>No-Arbitrage Deep Calibration for Volatility Smile and Skewness</li><li>Approximation Rates for Deep Calibration of (Rough) Stochastic Volatility Models</li><li>Applying Deep Learning to Calibrate Stochastic Volatility Models</li><li>Joint Calibration of Local Volatility Models with Stochastic Interest Rates using Semimartingale Optimal Transport<li>A new encoding of implied volatility surfaces for their synthetic generation</li><li>Calibrating Local Volatility Models with Stochastic Drift and Diffusion</li><li>Random neural networks for rough volatility</li><li>Calibration of Local Volatility Models with Stochastic Interest Rates using Optimal Transport</li><li>Learning Volatility Surfaces using Generative Adversarial Networks </ul></td></tr><tr><td><b>Cluster 7: Optimal portfolios, Stochastic volatility, Stackelberg game, Robust replication, Hedging</b><ul><li>Analysis of optimal portfolios on finite and small-time horizons for a multi-dimensional correlated stochastic volatility model</li><li>A Stackelberg reinsurance-investment game under Î±-maxmin mean-variance criterion and stochastic volatility</li><li>Ergodic robust maximization of asymptotic growth under stochastic volatility<li>Optimal Investment with Correlated Stochastic Volatility Factors</li><li>CVA in fractional and rough volatility models</li><li>Safe Delivery of Critical Services in Areas with Volatile Security Situation via a Stackelberg Game Approach</li><li>Can Volatility Solve the Naive Portfolio Puzzle?</li><li>Robust replication of barrier-style claims on price and volatility</li><li>Pricing Interest Rate Derivatives under Volatility Uncertainty<li>Term Structure Modeling under Volatility Uncertainty</li></ul></td></tr>
</table>


The work 'volatility' is in all clusters, but this shouldn't surprise given that we have selected the articles that contain it in the title! Five clusters reference 'stochastic volatility', although from different angles: cluster 1 from a modeling perspective (the SDE formulation), cluster 3 applied to option pricing, cluster 4 refers more to the smile representation, cluster 6 points to papers applying machine learning and option pricing, and cluster 7 refers to portfolio theory and portfolio selection. The distinction isn't always clear, suggesting that the clustering algorithm could be improved or fine-tuned.
