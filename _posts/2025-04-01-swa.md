---
layout: splash
permalink: /swa/
title: "On Stochastic Weight Averaging"
header:
  overlay_image: /assets/images/swa/swa.jpeg
excerpt: "A quick test of the effectiveness of stochastic weight averaging."
---

In this article we will explore stochastic weighted average method. We will use the [CoverType](https://archive.ics.uci.edu/dataset/31/covertype) dataset. The goal is to predict the forest cover type from cartographic variables only (no remotely sensed data).  The actual forest cover type for a given observation (30 x 30 meter cell) was determined from US Forest Service (USFS) Region 2 Resource Information System (RIS) data.  Independent variables were derived from data originally obtained from US Geological Survey (USGS) and USFS data.  Data is in raw form (not scaled) and contains binary (0 or 1) columns of data for qualitative independent variables (wilderness areas and soil types).

There are 7 forest cover types:
- Spruce/Fir;
- Lodgepole Pine;
- Ponderosa Pine;
- Cottonwood/Willow;
- Aspen;
- Douglas-fir;
- Krummholz.

The labels represent the dominant species of trees found on a 30m × 30m forest cell.


```python
import matplotlib.pylab as plt
plt.style.use('ggplot')
import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.datasets import fetch_covtype
data = fetch_covtype()
```


```python
df = pd.concat((
    pd.DataFrame(data.data, columns=data.feature_names),
    pd.DataFrame(data.target - 1, columns=data.target_names)
), axis='columns')
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Elevation</th>
      <th>Aspect</th>
      <th>Slope</th>
      <th>Horizontal_Distance_To_Hydrology</th>
      <th>Vertical_Distance_To_Hydrology</th>
      <th>Horizontal_Distance_To_Roadways</th>
      <th>Hillshade_9am</th>
      <th>Hillshade_Noon</th>
      <th>Hillshade_3pm</th>
      <th>Horizontal_Distance_To_Fire_Points</th>
      <th>...</th>
      <th>Soil_Type_31</th>
      <th>Soil_Type_32</th>
      <th>Soil_Type_33</th>
      <th>Soil_Type_34</th>
      <th>Soil_Type_35</th>
      <th>Soil_Type_36</th>
      <th>Soil_Type_37</th>
      <th>Soil_Type_38</th>
      <th>Soil_Type_39</th>
      <th>Cover_Type</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2596.0</td>
      <td>51.0</td>
      <td>3.0</td>
      <td>258.0</td>
      <td>0.0</td>
      <td>510.0</td>
      <td>221.0</td>
      <td>232.0</td>
      <td>148.0</td>
      <td>6279.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>4</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2590.0</td>
      <td>56.0</td>
      <td>2.0</td>
      <td>212.0</td>
      <td>-6.0</td>
      <td>390.0</td>
      <td>220.0</td>
      <td>235.0</td>
      <td>151.0</td>
      <td>6225.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>4</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2804.0</td>
      <td>139.0</td>
      <td>9.0</td>
      <td>268.0</td>
      <td>65.0</td>
      <td>3180.0</td>
      <td>234.0</td>
      <td>238.0</td>
      <td>135.0</td>
      <td>6121.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2785.0</td>
      <td>155.0</td>
      <td>18.0</td>
      <td>242.0</td>
      <td>118.0</td>
      <td>3090.0</td>
      <td>238.0</td>
      <td>238.0</td>
      <td>122.0</td>
      <td>6211.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2595.0</td>
      <td>45.0</td>
      <td>2.0</td>
      <td>153.0</td>
      <td>-1.0</td>
      <td>391.0</td>
      <td>220.0</td>
      <td>234.0</td>
      <td>150.0</td>
      <td>6172.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>4</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 55 columns</p>
</div>




```python
missing_data = df.isnull().sum()
assert len(missing_data[missing_data > 0]) == 0
```


```python
missing_data = df.isna().sum()
assert len(missing_data[missing_data > 0]) == 0
```


```python
numeric_cols = df.select_dtypes(include=[np.number]).columns

numeric_cols = [col for col in numeric_cols if not df[col].isin([0, 1]).all() and col != 'Cover_Type']
n_cols = len(numeric_cols)

assert n_cols > 0, "No numeric columns to visualize"

n_rows = (n_cols + 2) // 3
fig, axes = plt.subplots(n_rows, 3, figsize=(15, 5 * n_rows))
axes = axes.flatten() if n_rows > 1 else [axes]

target = 'Cover_Type'
for i, col in enumerate(numeric_cols):
    sns.boxplot(data=df, x=target, y=col, ax=axes[i])
    axes[i].set_title(f'{col}')

for i in range(n_cols, len(axes)):
    axes[i].set_visible(False)

plt.tight_layout()
```


    
![png](/assets/images/swa/swa-1.png)
    



```python
n_rows = (n_cols + 2) // 3
fig, axes = plt.subplots(n_rows, 3, figsize=(15, 5 * n_rows))
axes = axes.flatten() if n_rows > 1 else [axes]

for i, col in enumerate(numeric_cols):
    axes[i].hist(df[col], bins=30, alpha=0.7, color='skyblue', edgecolor='black')
    axes[i].set_title(f'Distribution of {col}')
    axes[i].set_xlabel(col)
    axes[i].set_ylabel('Frequency')

for i in range(len(numeric_cols), len(axes)):
    axes[i].set_visible(False)

plt.tight_layout()
```


    
![png](/assets/images/swa/swa-2.png)
    



```python
numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
numeric_cols_to_categoric = [col for col in numeric_cols if df[col].isin([0, 1]).all()]

categoric_cols = df.select_dtypes(include=['object']).columns.tolist()
onehot_cols = categoric_cols + numeric_cols_to_categoric

result_data = []
for col in onehot_cols:
    percentages = (
        df.groupby(target)[col]
            .mean()
            .reindex(range(1, 8), fill_value=0)
            .values
    )
    result_data.append(percentages)

result_df = pd.DataFrame(
    result_data, 
    index=onehot_cols, 
    columns=[str(i) for i in range(0, 7)]
)

plt.figure(figsize=(8, len(onehot_cols) * 0.5 + 2))
sns.heatmap(result_df, annot=True, fmt='.4f', cmap='Blues')
plt.xlabel('Target Class')
plt.ylabel('OneHot Variable')
plt.title('Percentage of 1s by OneHot Variable and Target Class')
plt.tight_layout()
```


    
![png](/assets/images/swa/swa-3.png)
    


The problem is heavily unbalanced, with almost half of the entries on class #1. This means that a model that always predicts #1 will achieve 50% accuracy.


```python
for k, v in df.Cover_Type.value_counts().sort_index().items():
    print(f"Class {k}: {v:>8,} entries ({v / len(df):.2%})")
```

    Class 0:  211,840 entries (36.46%)
    Class 1:  283,301 entries (48.76%)
    Class 2:   35,754 entries (6.15%)
    Class 3:    2,747 entries (0.47%)
    Class 4:    9,493 entries (1.63%)
    Class 5:   17,367 entries (2.99%)
    Class 6:   20,510 entries (3.53%)



```python
from sklearn.model_selection import train_test_split

X = data.data.astype(np.float32)
y = (data.target - 1).astype(np.int64)

# train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
```


```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train).astype(np.float32)
X_test  = scaler.transform(X_test).astype(np.float32)
```


```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.swa_utils import AveragedModel, SWALR
from torch.utils.data import TensorDataset, DataLoader
from sklearn.metrics import balanced_accuracy_score

train_ds = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))
test_ds  = TensorDataset(torch.tensor(X_test), torch.tensor(y_test))

train_loader = DataLoader(train_ds, batch_size=1_024, shuffle=True)
test_loader  = DataLoader(test_ds, batch_size=1_024)
```


```python
def get_device():
    if torch.cuda.is_available():
        return 'cuda'
    if torch.mps.is_available():
        return 'mps'
    return 'cpu'
```


```python
class MLP(nn.Module):

    def __init__(self):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(54, 512), nn.ReLU(),
            nn.BatchNorm1d(512),
            nn.Linear(512, 512), nn.ReLU(),
            nn.BatchNorm1d(512),
            nn.Linear(512, 256), nn.ReLU(),
            nn.BatchNorm1d(256),
            nn.Linear(256, 7)
        )
    def forward(self, x):
        return self.model(x)

device = get_device()
model = MLP().to(device)
```


```python
criterion = nn.CrossEntropyLoss()

optimizer = optim.SGD(
    model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4
)

epochs = 200
swa_start = 30

scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=swa_start)

swa_model = AveragedModel(model).to(device)
swa_scheduler = SWALR(optimizer, swa_lr=0.05)

def evaluate(model, loader, device="cuda"):
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for Xb, yb in loader:
            Xb, yb = Xb.to(device), yb.to(device)
            preds = model(Xb).argmax(1)

            all_preds.append(preds.cpu().numpy())
            all_labels.append(yb.cpu().numpy())

    all_preds = np.concatenate(all_preds)
    all_labels = np.concatenate(all_labels)

    overall_acc = (all_preds == all_labels).mean()
    balanced_acc = balanced_accuracy_score(all_labels, all_preds)

    # Per-class accuracy (optional)
    class_acc = {}
    for cls in np.unique(all_labels):
        mask = (all_labels == cls)
        class_acc[int(cls)] = (all_preds[mask] == all_labels[mask]).mean()

    return overall_acc, balanced_acc, list(map(float, class_acc.values()))

lrs, sgd_accs, class_accs = [], [], []

from tqdm import trange
for epoch in (p := trange(epochs)):
    model.train()
    for Xb, yb in train_loader:
        Xb, yb = Xb.to(device), yb.to(device)
        optimizer.zero_grad()
        loss = criterion(model(Xb), yb)
        loss.backward()
        optimizer.step()

    # update scheduler
    if epoch > swa_start:
        scheduler.step()
        lrs.append(scheduler.get_last_lr())
    else:
        swa_model.update_parameters(model)
        swa_scheduler.step()
        lrs.append(swa_scheduler.get_last_lr())

    overall_acc, balanced_acc, class_acc = evaluate(model, test_loader, device)
    sgd_accs.append(balanced_acc)
    class_accs.append(class_acc)
    class_acc = ','.join(map(lambda x: f'{x:.2%}', class_acc))
    p.set_description(f"Epoch {epoch+1}/{epochs}: {overall_acc:.4f}/{balanced_acc:.4f}/{class_acc}")

torch.optim.swa_utils.update_bn(train_loader, swa_model, device=device)
swa_overall_acc, swa_balanced_acc, swa_class_acc = evaluate(swa_model, test_loader, device)

print(f"SWA accuracy: {swa_overall_acc:.4f}/{swa_balanced_acc:.4f}/{','.join(map(lambda x: f'{x:.2%}', swa_class_acc))}")
```

    Epoch 200/200: 0.9283/0.8536/91.18%,95.55%,91.64%,68.82%,68.82%,88.08%,93.45%: 100%|██████████| 200/200 [10:40<00:00,  3.20s/it]


    SWA accuracy: 0.9563/0.9239/95.30%,96.33%,96.05%,81.94%,87.62%,93.44%,96.04%



```python
plt.plot(sgd_accs, label='SGD')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title("Test Dataset Accuracy")
plt.axhline(y=swa_balanced_acc, linestyle='dashed', color='green', label='SWA')
plt.legend();
```


    
![png](/assets/images/swa/swa-4.png)
    



```python
class_accs = np.array(class_accs)
fig, axes = plt.subplots(figsize=(12, 8), nrows=2, ncols=4, sharex=True, sharey=True)
for i, ax in enumerate(axes.ravel()):
    if i < 7:
        ax.plot(class_accs[:, i])
        ax.axhline(y=swa_class_acc[i], color='green', linestyle='dashed')
        ax.set_title(f'Feature #{i}')
    else:
        ax.axis('off')
fig.tight_layout()
```


    
![png](/assets/images/swa/swa-5.png)
    

