---
layout: splash
permalink: /fpca/
title: "A Quick Introduction to Functional Data Analysis"
header:
  overlay_image: /assets/images/fpca/fpca.jpeg
excerpt: "On the application of the Kahrunen-Loève decomposition to data analysis"
---

In this article we take a look at **functional data analysis**. In this branch of data analysis we assunme the observed data are generated by an underlying stochastic process $X(t) \in L_2([0, T])$ with finite second moments, that is the mean function $\mu(t) = \mathbb{E}[X(t)]$ and the correlation function $C(s, t) = \mathbb{E}[(X(s) - \mu(s))(X(t) - \mu(t))]$ are finite. In addition to the assumption of an underlying (stochastic) functional form, we also assume that this functional form is smooth — two adjancent data values are linked together and unlikely to be different. Often, this implies that our underlying process $X$ has one or more derivatives. (The observed data may not be smooth because of measurement errors or noise.)

The mathematical tool that underpins what we will do is the Kahrunen-Loève (KL) decomposition. The KL decomposition is similar to the Fourier decomposition but acts on a continuous parameter random function $X(t), t \in [a, b]$. Of all the possible decompositions of such process, the KL decomposition is the one that minimizes the total mean square error. It can be defined as follows: $\forall t \in [a, b]$,

$$
X(t) = \mu(t) + \sum_{k=1}^{\infty} Z_k \, \varphi_k(t),
$$

with

$$
Z_k = \int_a^b (X(t) - \mu(t)) \, \varphi_k(t) dt
$$

where the $\{ \varphi_k(t), k=1, \ldots, \infty \}$ are the eigenfunction of the covariance operator and the $\{ c_k \}$ the pairwise orthogonal random variables with zero mean and variance $\lambda_k$, where $\lambda_k$ is the eigenvalue corresponding to the $k$-th eigenfunction $\varphi_k(t)$. We also have $\mathbb{E}[Z_K] = 0$ and $\mathbb{V}[Z_k] = \lambda_k$.
The series $\sum_{k=1}^{\infty} Z_k \, \varphi_k(t)$ converges in mean square, uniformly in $t \in [a, b]$. The terms $Z_k$ are called the **scores**.

Karhunen-Loève Decomposition (KLD) is a widely used technique in functional data analysis (FDA) for representing and analyzing functional data. It is a dimensionality reduction method that transforms high-dimensional functional data into a lower-dimensional representation, retaining most of the variability in the data.

In an alternative form, we can define 

$$
Z_k^\star = \frac{Z_k}{\sqrt{\lambda_k}},
$$

obtaining

$$
X(t) = \mu(t) + \sum_{k=1}^\infty \sqrt{\lambda_k} Z_k^\star \, \varphi_k(t),
$$

where $\mathbb{E}[Z_k^\star] = 0$ and $\mathcal{V}[Z_k^\star] = 1$. By using this form we can quickly evalute the variances,

$$
\mathbb{V}[X(t)] = \mathbb[X(t)^2] - \mathbb{E}[X(t)]^2 = \sum_{k=1}^\infty \lambda_k \, \varphi_k(t)^2,
$$

and consequently the total variance of the process over $[a, b]$ is

$$
\begin{align}
\int_a^b \mathbb{V}[X(t)] dt & = \int_a^b \sum_{k=1}^\infty \lambda_k \, \varphi_k(t)^2 dt \\
%
& = \sum_{k=1}^\infty \int_a^b \lambda_k \, \varphi_k(t)^2 dt \\
%
& = \sum_{k=1}^\infty \lambda_k \int_a^b \varphi_k(t)^2 dt \\
%
& = \sum_{k=1}^\infty \lambda_k,
\end{align}
$$

since the convergence is the series is uniform by Mercer's theorem. If the expansion is truncated after $N$ terms, we would explain

$$
\sum_{k=1}^N \lambda_k / \sum_{k=1}^\infty \lambda_k
$$

of the total variance of the process.

The KL decomposition is a parsimonious description of functional data as it is the linear combination of the basis functions that explains the highest fraction of variance in the data with a given number of components. Its main attraction is the equivalence between $X(t)$ and the scores $\{Z_1, Z_2, \ldots\}$ such that any $X(t)$ can be expressed in terms of a countable sequence of uncorrelated scores, in decreasing order of importance (covariance-wise).

In order to compute such an approximation numerically, the first step is the definition of a set of basis functions, that is the functions $\{ \varphi_k(t), k=1, \ldots, \infty \}$ above that are independent of each other and that can approximate arbitratrily well any functions by taking a weighted sum of a large number of such functions. Several choices are available:
- *polynomials* are easy to define and compute but tend to be oscillate and produce unstable results with higher orders;
- *Fourier basis functions* have excellent computational properties, especially if the observations are equally spaced, and are natural for describing periodic data, but they can be problematic if the data is not periodic;
- *finite element basis functions* can be very convenient for domain with complicated shapes; and
- *splines* combine the fast computations of polynomials with substantilly greater flexibility, often achieved with only a modest number of basis functions.

We will focus on splines, which are defined by the range of validity, the knots, and the degree. There are many different kinds of splines; here we consider [B-splines](https://fr.wikipedia.org/wiki/B-spline). B-splines are constructed by dividing the interval of validity into subintervals, with boundaries at points called "break points" or "knots". In every subinterval the spline function is a polynomial of fixed degree; generally the degree is the same in every subinterval. At each knot, neighboring polynomials are constraint to have a certain regularity, enforcing continuity up to a certain order of derivation. Often, splines are defined to be continuous up to the second derivative.

Let's first look at the construction of the basis functions using Python's [scikit-fda](https://github.com/GAA-UAM/scikit-fda) package. The locality of B-spline basis functions is apparent and constitues a strong incentive to use them for non-periodic data.


```python
import matplotlib.pylab as plt
import numpy as np
import skfda
```


```python
fig, (ax0, ax1) = plt.subplots(figsize=(10, 4), ncols=2)

basis = skfda.representation.basis.FourierBasis([0, 1], n_basis=5)
x = np.linspace(0, 1, 101)
y = basis(x).reshape(5, 101)
for i in range(5):
    ax0.plot(x, y[i, :], label=fr'$\varphi_{i + 1}$', linewidth=4)
ax0.legend()
ax0.set_title('Fourier Basis Functions')

basis = skfda.representation.basis.BSplineBasis(n_basis=5)
x = np.linspace(0, 1, 101)
y = basis(x).reshape(5, 101)
for i in range(5):
    ax1.plot(x, y[i, :], label=fr'$\varphi_{i + 1}$', linewidth=4)
ax1.legend()
ax1.set_title('B-Spline Basis Functions');
```


    
![png](/assets/images/fpca/fpca-1.png)
    


In multiple dimensions, the easiest approach is to combine single-dimension functions in a tensor basis. We plot a few of the resulting basis functions.


```python
basis = skfda.representation.basis.TensorBasis([
    skfda.representation.basis.BSplineBasis(n_basis=4, domain_range=[0, 1]),
    skfda.representation.basis.BSplineBasis(n_basis=4, domain_range=[0, 2]),
])

XX, YY = plt.meshgrid(np.linspace(0, 1, 101), np.linspace(0, 2, 101))
ZZ = basis(np.vstack((XX.flatten(), YY.flatten())).T).reshape(16, 101 * 101)
```


```python
fig = plt.figure(figsize=(9, 7))
axes = [fig.add_subplot(230 + k, projection='3d') for k in range(1, 7)]
for i, ax in enumerate(axes):
    ax.plot_surface(XX, YY, ZZ[i].reshape(101, 101))
    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    ax.set_title(rf'$\varphi_{i + 1}$')
    ax.set_zlim(0, 1)
fig.tight_layout()
```


    
![png](/assets/images/fpca/fpca-2.png)
    


As an application, we consider the Kahrunen-Loève decomposition of a Wiener process $W(t)$ in $[0, 1]$, which can be computed and reads

$$
W(t) = \frac{2 \sqrt{2}}{\pi} \sum_{k=1}^\infty \frac{Z_k^\star}{2k - 1} \sin \left( \frac{2(k -1) \pi}{2} t \right)
$$

where

$$
\begin{align}
Z_k & = \int_0^1 W(t)\sqrt{2} \sin\left( \frac{2(k-1)\pi}{2} t \right) dt \\
%
\lambda_k & = \frac{4}{(2k - 1)^2 \pi^2} \\
%
Z_k^\star & = \frac{Z_k}{\sqrt{\lambda_k}}
\end{align}
$$

and the convergence in mean square is almost surely.


```python
T = 1.0
num_steps = 100
sqrt_dt = np.sqrt(T / (num_steps + 1))
num_paths = 10000
X = np.zeros((1, num_paths))
X_all = [X]
for i in range(num_steps):
    W = np.random.randn(num_paths)
    X = X + sqrt_dt * W
    X_all.append(X)
X_all = np.concatenate(X_all).T
cov = np.cov(X_all.T)
```

As expected, the variance grows linearly with $t$ and the covariance function is given by $\min(s, t)$.


```python
fig = plt.figure(figsize=(8, 4))
ax0 = fig.add_subplot(121)
t_axis = np.linspace(0, T, num_steps + 1)
ax0.plot(t_axis, X_all.var(axis=0))
ax0.set_xlabel('t')
ax0.set_ylabel('Variance')

ax1 = fig.add_subplot(122, projection='3d')
XX, YY = plt.meshgrid(t_axis, t_axis)
ax1.plot_surface(XX, YY, cov)
ax1.set_xlabel('s')
ax1.set_ylabel('t')
ax1.set_title('Covariance(s, t) = min(s, t)');
```


    
![png](/assets/images/fpca/fpca-3.png)
    


First we do the decomposition using principal component analysis. The result is not bad, however the scale isn't correct.


```python
from sklearn.decomposition import PCA
pca = PCA(n_components=4)
pca.fit(X_all)

fig, (ax0, ax1) = plt.subplots(figsize=(8, 4), ncols=2)
ax0.plot(pca.explained_variance_ratio_)
ax0.set_xlabel('Component')
ax0.set_title('Explained Variance Ratio')

for i in range(4):
    ax1.plot(t_axis, pca.components_[i], label=f'Component {i}')
ax1.legend()
ax1.set_xlabel('t')
ax1.set_title('Components');
```


    
![png](/assets/images/fpca/fpca-4.png)
    


The KL decomposition using B-spllines, instead, provides an accurate representation of the basis functions.


```python
fd = skfda.FDataGrid(
    data_matrix=X_all,
    grid_points=t_axis,
)

basis = skfda.representation.basis.BSplineBasis(n_basis=10)
basis_fd = fd.to_basis(basis)

from skfda.preprocessing.dim_reduction import FPCA
fpca = FPCA(n_components=4)
fpca.fit(basis_fd)
fpca.components_.plot();
```


    
![png](/assets/images/fpca/fpca-5.png)
    

