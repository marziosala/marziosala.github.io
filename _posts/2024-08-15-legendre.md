---
layout: splash
permalink: /legendre/
title: "Functional Data Analysis, Redux"
header:
  overlay_image: /assets/images/legendre/legendre.png
excerpt: "Another approach for functional data analysis, this time using Legendre polynomials."
---

In this article we continue the exploration of functional principal component analysis (FPCA) that we did [before](/fpca). We start by loooking deeper into [Mercer's theorem](https://en.wikipedia.org/wiki/Mercer%27s_theorem). The theorem states that every continuous, symmetric positive-definite function $K(x, y) : X \times X \rightarrow \mathbb{R}$, with $X \in \mathbb{R}^d$, can be expressed as an infinite sum of product functions,

$$
K(x, y) = \sum_{i=1}^\infty \lambda_i \varphi_i(x) \varphi_i(y),
$$

where the $\lambda_i$ are non-negative eigenvalues and the $\varphi_i$ the corresponding eigenfunctions. The convergence is absolute and uniform.

The function $K(x, y)$, is called a **kernel function**, that is a function that is symmetric, $K(x, y) = K(y, x), \forall x, y \in X$ and is positive definite, meaning that for any finite set of points $\{x_1, x_2, \ldots, x_d \}$ and any real numbers $\{ c_1, c_2, \ldots, c_n \}$, the inequality
$$
\sum_{i, j} c_i c_j K(x_i, x_j) \ge 0
$$
holds. This means that the [Gram](https://en.wikipedia.org/wiki/Gram_matrix) matrix $G \in \mathbb{R}^{d \times d}$ with elements $G_{i, j} = K(x_i, x_j)$ is symmetric positive definite.

Common examples of kernel functions are, $\forall x, y \in \mathbb{R}^d$,
- the linear kernel $K(x, y) = x^T y$,
- the polynomial kernel $K(x, y) = ()$, with $r \ge 0, n\ge 1$, and
- the Gaussian kernel $K(x, y) = e^{ -\frac{\lVert x - y\rVert^2}{2 \sigma^2} }, \sigma > 0.$

A simple example to look at takes $X = [a, b] \subset \mathbb{R}$ and $K(x, y) = f(x) f(y)$. An eigenfunction $\varphi(x)$ must satisfy

$$
\begin{aligned}
\int_a^b K(x, y) \varphi(y) dy & = \lambda \varphi(x) \\
%
\int_a^b f(x) f(y) \varphi(y) dy & = \lambda \varphi(x) \\
%
f(x) \int_a^b f(y)\varphi(y) dy & = \lambda \varphi(x).
\end{aligned}
$$

Assuming $\lambda \neq 0$, we must have $\varphi(x) = \alpha f(x)$ for some $x \in \mathbb{R}$, and consequently

$$
f(x) \int_a^b f(y) \alpha f(y) dy = \lambda \alpha f(x)
$$

which yields

$$
\lambda = \int_a^b f(y)^2 dy.
$$

To get the normalized eigenfunctions, we need

$$
\int_a^b \alpha^2 f(y)^2 dy = 1,
$$

that is
$$
\alpha = \left(  \int_a^b f(y)^2 dy \right)^{-1/2}.
$$

It also follows that
$$
\begin{aligned}
\lambda \varphi(x) \varphi(y) & = \lambda \lambda^{-1/2} f(x) \lambda^{-1/2} f(y) \\
& = f(x) f(y) \\
& = K(x, y), 
\end{aligned}
$$

as expected from Mercer's theorem, since there is only one eigenvalue (and eigenfunction).

In the case $f(x) = x$, it is easy to see that

$$
\lambda = \int_a^b y^2 dy = \frac{1}{3}(b-a)^3
$$

and

$$
\alpha = \left( y^2 dy \right)^{-1/2} = \lambda^{-1/2}.
$$

Therefore,
$$
\varphi(x) = \lambda^{-1/2} x.
$$

After this small detour on Mercer's theorem, we go back to the [Karhunen-Lo√®ve Decomposition](https://en.wikipedia.org/wiki/Kosambi%E2%80%93Karhunen%E2%80%93Lo%C3%A8ve_theorem) introduced in the previous article, focusing on two-dimensional . We define the problem, let's consider a sequence
